{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a620de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import SpatialDropout2D\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras import backend as K\n",
    "import keras\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics \n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from wandb.keras import WandbCallback\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d9188",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_test=8\n",
    "D_test=2\n",
    "F2_test=16\n",
    "Drop_test=0 #originally 0.5\n",
    "KernLength_test=64 #Originally 64 \n",
    "batch_test=16\n",
    "\n",
    "N_chan=31\n",
    "N_samples_long=250\n",
    "# N_samples_short=251\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "MIN_LOSS_FOUND=100\n",
    "MAX_R2_FOUND=-100\n",
    "MAX_CORR_COEF_FOUND=-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac484edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epochs, logs=None):\n",
    "        old_weights=[]\n",
    "        for idx in range(8):\n",
    "            old_weights.append(model.trainable_variables[0].numpy()[0,:,0,idx])\n",
    "      \n",
    "        old_weights=np.stack(old_weights)\n",
    "        lista.append(old_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8110a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SINGLE ROI\n",
    "SUBJECTS=[1,2,3,4,5,6,7,8,9,11,12,13,14,16,17]\n",
    "for temp_sub in SUBJECTS:\n",
    "    lista=[]\n",
    "    sub=\"{:02d}\".format(temp_sub)\n",
    "\n",
    "    #Define loss function\n",
    "    loss_fn= tf.keras.losses.MeanSquaredError()\n",
    "    evaluation=[]\n",
    "    iteration=[]\n",
    "    confusion_matrix_x_test=[]\n",
    "    confusion_matrix_y_test= []\n",
    "    validation_acc=[]\n",
    "    MSE_filters_mean=[]\n",
    "    NUM_ROI=1\n",
    "\n",
    "    print(\"SUBJECT: \"+ str(sub))\n",
    "    \n",
    "    X=np.load(\"input/sub_\"+str(sub)+ \"_compiled_RAW_downsampled_float32.npy\")\n",
    "    y= sio.loadmat(\"FINAL_regression/right_SMA/input/sub-\"+str(sub)+\"_compiled_bold_non_norm_z_score_run_clipped.mat\")[\"compiled_bold_non_norm_z_score_run_clipped\"]\n",
    "\n",
    "    X= X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "    y=y.flatten()\n",
    "    print(\"New shape for X: \" + str(X.shape))\n",
    "    print(\"New shape for y: \"+str(y.shape))\n",
    "\n",
    "    dir0= os.path.join(\"FINAL_regression/right_SMA/output\", \"sub_\"+str(sub))\n",
    "    os.mkdir(dir0)\n",
    "    dir1=os.path.join(dir0, \"comparison\")\n",
    "    dir2=os.path.join(dir0, \"temporal_convolution\")\n",
    "    dir3=os.path.join(dir0, \"spatial_convolution\")\n",
    "    os.mkdir(dir1)\n",
    "    os.mkdir(dir2)\n",
    "    os.mkdir(dir3)\n",
    "\n",
    "    ################################################################################################################################################################################################################################################################################\n",
    "    Fs=125\n",
    "    EPOCHS=800\n",
    "    lr_test=1e-4\n",
    "    batch_test=64\n",
    "    decay_test=0\n",
    "    kern_test=0.15\n",
    "\n",
    "    n_folds = 5\n",
    "    seed = 21\n",
    "    shuffle_test = True\n",
    "    r2=[]\n",
    "    adj_r2=[]\n",
    "    corr_coef=[]\n",
    "    final_val_loss=[]\n",
    "    stats=[]\n",
    "\n",
    "    kfold = KFold(n_splits = n_folds, shuffle = shuffle_test, random_state = seed)\n",
    "    count=0\n",
    "    sommatoria=0\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        count=count+1\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        print(\"Train index for this split: \"+ str(train_index)) \n",
    "        print(\"Number of samples for train set: \"+str(train_index.shape[0]))\n",
    "        print(\"Test index for this split: \"+ str(test_index))\n",
    "        print(\"Number of samples for test set: \"+str(test_index.shape[0]))\n",
    "\n",
    "        # Define the model architecture - \n",
    "\n",
    "        model=Sequential()\n",
    "\n",
    "        ##################################################################\n",
    "\n",
    "        model.add(Conv2D(8, (1, 64), padding = 'same',\n",
    "                                       input_shape = (N_chan, N_samples_long, 1),\n",
    "                                       use_bias = False, name=\"temporal_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_1\"))\n",
    "        model.add(DepthwiseConv2D((31, 1), use_bias = False, \n",
    "                                       depth_multiplier = 2,\n",
    "                                       depthwise_constraint = max_norm(1.), name=\"spatial_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_2\"))\n",
    "        model.add(Activation('elu', name=\"activation_1\"))\n",
    "        model.add(AveragePooling2D((1, 4), name=\"pooling_layer_1\"))\n",
    "        model.add(Dropout(0.5, name=\"dropout_1\"))\n",
    "\n",
    "        model.add(SeparableConv2D(16, (1, 16),\n",
    "                                       use_bias = False, padding = 'same', name=\"separable_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_3\"))\n",
    "        model.add(Activation('elu', name=\"activation_2\"))\n",
    "        model.add(AveragePooling2D((1, 8), name=\"pooling_layer_2\"))\n",
    "        model.add(Dropout(0.5, name=\"drpout_2\")) #QUI DROPOUT E' LASCIATO A 0.5 come in eegnet paper\n",
    "\n",
    "        model.add(Flatten(name = 'flatten'))\n",
    "\n",
    "        model.add(Dense(NUM_ROI, name = 'dense',\n",
    "                             kernel_constraint = max_norm(kern_test)))\n",
    "#     model.add(Activation('softmax', name = 'softmax'))\n",
    "\n",
    "\n",
    "          # Define the optimizer\n",
    "        optimizer= optimizers.Adam(\n",
    "        learning_rate= lr_test,\n",
    "        weight_decay= decay_test\n",
    "        )\n",
    "        model.compile(optimizer=optimizer,\n",
    "                       loss=loss_fn,\n",
    "                       metrics=[RSquare()])\n",
    "\n",
    "        evaluation.append(model.fit(X_train, y_train, batch_size=batch_test,\n",
    "                  epochs=EPOCHS, \n",
    "                  validation_data=(X_test, y_test), \n",
    "                  verbose=2, workers=1, \n",
    "                  callbacks=[CustomCallback()]))\n",
    "\n",
    "        iteration.append(model)\n",
    "        #SALVO R2 e Adjusted R2 score per fold\n",
    "        modello= model.predict(X_test)\n",
    "\n",
    "        temp_r2=[]\n",
    "        temp_adj_r2=[]\n",
    "        temp_r2=r2_score(y_test, modello)\n",
    "\n",
    "        n= y_test.shape[0]\n",
    "        p= N_chan*N_samples_long\n",
    "        temp_adj_r2= 1-((1-temp_r2)*((n-1)/(n-p)))\n",
    "\n",
    "        temp_corr_coef=[]\n",
    "        modello=modello.flatten()\n",
    "        temp_corr_coef=np.corrcoef(y_test, modello)[0,1]\n",
    "\n",
    "        r2.append(temp_r2)\n",
    "        adj_r2.append(temp_adj_r2)\n",
    "        corr_coef.append(temp_corr_coef)\n",
    "\n",
    "        # PLOTTO EFFETTIVA REGRESSIONE VISIVAMENTE\n",
    "        plt.figure()\n",
    "        plt.figure(figsize=(25,5))\n",
    "        plt.plot(y_test)\n",
    "        plt.plot(model.predict(X_test))\n",
    "    #     plt.plot(y_test-model.predict(X_test))\n",
    "        plt.savefig(dir1+\"/comparison_kfold_\"+str(count)+\"_zscored_run_clipped\")\n",
    "\n",
    "        var= (model.get_layer(\"temporal_conv\").weights)\n",
    "        for lallo in range(8):\n",
    "            plt.figure()\n",
    "            plt.title(\"temp_conv_\"+str(lallo))\n",
    "            plt.plot(var[0][0,:,0][:,lallo]) #this way i access the temporal filters, cambiando ultimo zero\n",
    "            plt.savefig(dir2+\"/ALL_temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm\")\n",
    "            temp= (var[0][0,:,0][:,lallo]).numpy()\n",
    "            sio.savemat(dir2+\"/ALL_temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm.mat\", {\"array\": temp})\n",
    "            plt.close()\n",
    "\n",
    "            neil_x, neil_y=sp.signal.welch(model.trainable_variables[0].numpy()[0,:,0,lallo], Fs, nperseg=64)\n",
    "            plt.figure()\n",
    "            plt.title(\"Spectrogram_\"+str(lallo))\n",
    "            plt.plot(neil_x, neil_y)\n",
    "            plt.savefig(dir2+\"/ALL_spectrogram_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm\")\n",
    "            plt.close()\n",
    "\n",
    "        var_2= (model.get_layer(\"spatial_conv\").weights)\n",
    "        reshaped_var_2=tf.reshape(var_2[0][:,0,:,:],(31,16))\n",
    "        for lallo in range(16):\n",
    "            nump= reshaped_var_2[:,lallo].numpy()\n",
    "            sio.savemat(dir3+\"/ALL_spat_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm.mat\", {\"array\": nump})\n",
    "\n",
    "    # print(\"--------------------------------------- AVERAGE MEAN AND VARIANCE ACROSS FOLDS ---------------------------------------\")\n",
    "    # print(np.mean(val_mean_squared_error))\n",
    "    # print(np.var(val_mean_squared_error))\n",
    "\n",
    "    # PLOTTO LOSS E VALIDATION LOSS, MEAN SQUARED ERROR E VALIDATION MEAN SQUARED ERROR\n",
    "\n",
    "    #plot accuracy and loss function across epochs\n",
    "    epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "    min_temp_loss=10\n",
    "    min_temp_acc=10\n",
    "    max_temp_loss=0\n",
    "    max_temp_acc=0\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        if (np.min(evaluation[idx].history['loss'])<min_temp_loss):\n",
    "            min_temp_loss=np.min(evaluation[idx].history['loss'])\n",
    "        if (np.min(evaluation[idx].history['val_loss'])<min_temp_loss):\n",
    "            min_temp_loss=np.min(evaluation[idx].history['val_loss'])\n",
    "        if (np.max(evaluation[idx].history['loss'])>max_temp_loss):\n",
    "            max_temp_loss=np.max(evaluation[idx].history['loss'])\n",
    "        if (np.max(evaluation[idx].history['val_loss'])>max_temp_loss):\n",
    "            max_temp_loss=np.max(evaluation[idx].history['val_loss'])\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        if (np.min(evaluation[idx].history['r_square'])<min_temp_acc):\n",
    "            min_temp_acc=np.min(evaluation[idx].history['r_square'])\n",
    "        if (np.min(evaluation[idx].history['val_r_square'])<min_temp_acc):\n",
    "            min_temp_acc=np.min(evaluation[idx].history['val_r_square'])\n",
    "        if (np.max(evaluation[idx].history['r_square'])>max_temp_acc):\n",
    "            max_temp_acc=np.max(evaluation[idx].history['r_square'])\n",
    "        if (np.max(evaluation[idx].history['val_r_square'])>max_temp_acc):\n",
    "            max_temp_acc=np.max(evaluation[idx].history['val_r_square'])\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        loss_vec_train= evaluation[idx].history['loss']\n",
    "        loss_vec_test= evaluation[idx].history['val_loss']\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "        plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss across epochs for kfold: '+str(idx))\n",
    "        plt.ylim([min_temp_loss, max_temp_loss])\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(dir1+\"/loss_kfold_\"+str(idx))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    #plot accuracy and loss function across epochs\n",
    "    epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "\n",
    "    # inizio=0;\n",
    "    # for idx in range(n_folds):\n",
    "    #     mse_filter_min= min(MSE_filters_mean)\n",
    "    #     mse_filter_max= max(MSE_filters_mean)\n",
    "    #     fine=(idx+1)*EPOCHS\n",
    "\n",
    "    #     plt.figure()\n",
    "    #     plt.plot(MSE_filters_mean[inizio:fine],'b-');\n",
    "\n",
    "    #     plt.xlabel('Epoch')\n",
    "    #     plt.ylabel('mean mse filters')\n",
    "    #     plt.title('mean_mse_temporal_filters across epochs for kfold: '+str(idx))\n",
    "    #     plt.ylim([mse_filter_min, mse_filter_max])\n",
    "    # #     plt.legend()\n",
    "\n",
    "    #     plt.savefig(dir1+\"/mean_mse_filters_kfold_\"+str(idx))\n",
    "    #     plt.close()\n",
    "    #     inizio=fine\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        loss_vec_train= evaluation[idx].history['r_square']\n",
    "        loss_vec_test= evaluation[idx].history['val_r_square']\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "        plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('r_square')\n",
    "        plt.title('r_square across epochs for kfold: '+str(idx))\n",
    "        plt.ylim([min_temp_acc, max_temp_acc])\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(dir1+\"/r_square_kfold_\"+str(idx))\n",
    "        plt.close()\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        temp_val_loss=[]\n",
    "        temp_val_loss= evaluation[idx].history['val_loss'][-1] #with this indexing i select the last element of the list\n",
    "        final_val_loss.append(temp_val_loss)\n",
    "\n",
    "    gianfranco= model.predict(X)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.figure(figsize=(25,5))\n",
    "    plt.plot(y, 'b', label='true')\n",
    "    plt.plot(gianfranco, 'r', label='predicted')\n",
    "    plt.legend()\n",
    "    plt.savefig(dir1+\"/total_comparison_zscored_run_clipped\")\n",
    "    plt.savefig(dir0+\"/total_comparison_zscored_run_clipped\")\n",
    "\n",
    "    sio.savemat(dir1+\"/regression_results_zscored_run_clipped.mat\", {\"array\": gianfranco})\n",
    "\n",
    "    sio.savemat(dir1+\"/r2_score_zscored_run_clipped.mat\", {\"array\": r2})\n",
    "    sio.savemat(dir1+\"/adjusted_r2_score_zscored_run_clipped.mat\", {\"array\": adj_r2})\n",
    "    sio.savemat(dir1+\"/corr_coef_zscored_run_clipped.mat\", {\"array\": corr_coef})\n",
    "    sio.savemat(dir1+\"/final_val_loss_zscored_run_clipped.mat\", {\"array\": final_val_loss})\n",
    "\n",
    "    avg_final_loss= np.mean(final_val_loss)\n",
    "    var_final_loss=statistics.pstdev(final_val_loss)\n",
    "    avg_r2= np.mean(r2)\n",
    "    var_r2=statistics.pstdev(r2)\n",
    "    avg_corr_coef= np.mean(corr_coef)\n",
    "    var_corr_coef= statistics.pstdev(corr_coef)\n",
    "    \n",
    "    stats.append(avg_final_loss)\n",
    "    stats.append(var_final_loss)\n",
    "    stats.append(avg_r2)\n",
    "    stats.append(var_r2)\n",
    "    stats.append(avg_corr_coef)\n",
    "    stats.append(var_corr_coef)\n",
    "    sio.savemat(dir0+\"/stats_zscored_run_clipped.mat\", {\"array\": stats})\n",
    "    \n",
    "####################################################################################################################################\n",
    "\n",
    "    mse_epoca=[]\n",
    "    for epoca in range(EPOCHS*n_folds):\n",
    "        mse_filtro=[]\n",
    "        for filtro in range(8):\n",
    "            curr= lista[epoca][filtro][:]\n",
    "            precedente= lista[epoca-1][filtro][:]    \n",
    "            mse_filtro.append(((curr-precedente)**2).mean(axis=None))\n",
    "\n",
    "        mse_epoca.append(np.mean(mse_filtro))    \n",
    "        \n",
    "    inizio=0\n",
    "    for idx in range(n_folds):\n",
    "        mse_filter_min= min(mse_epoca)\n",
    "        mse_filter_max= max(mse_epoca)\n",
    "        fine=(idx+1)*EPOCHS\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(mse_epoca[inizio+1:fine],'b-');\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('mean mse filters')\n",
    "        plt.title('mean_mse_temporal_filters across epochs for kfold: '+str(idx))\n",
    "    #     plt.ylim([mse_filter_min, mse_filter_max])\n",
    "        plt.legend()\n",
    "        sio.savemat(dir1+\"/mean_mse_filter_kfold_\"+str(idx)+\".mat\", {\"array\": mse_epoca[inizio+1:fine]})\n",
    "\n",
    "        plt.savefig(dir1+\"/mean_mse_filters_kfold_\"+str(idx))\n",
    "        plt.close()\n",
    "        inizio=fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f21076",
   "metadata": {},
   "source": [
    "# ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad6ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epochs, logs=None):\n",
    "        old_weights=[]\n",
    "        for idx in range(8):\n",
    "            old_weights.append(model.trainable_variables[0].numpy()[0,:,0,idx])\n",
    "      \n",
    "        old_weights=np.stack(old_weights)\n",
    "        lista.append(old_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a6b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ALL ROI\n",
    "lista=[]\n",
    "\n",
    "#Define loss function\n",
    "loss_fn= tf.keras.losses.MeanSquaredError()\n",
    "evaluation=[]\n",
    "iteration=[]\n",
    "confusion_matrix_x_test=[]\n",
    "confusion_matrix_y_test= []\n",
    "validation_acc=[]\n",
    "MSE_filters_mean=[]\n",
    "NUM_ROI=1\n",
    "\n",
    "X=np.load(\"input/RAW_regression_all_subs_float32.npy\")\n",
    "y= sio.loadmat(\"FINAL_regression/right_SMA/input/compiled_bold_non_norm_all_z_score_run_clipped.mat\")[\"compiled_bold_non_norm_all_z_score_run_clipped\"]\n",
    "\n",
    "X= X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "y=y.flatten()\n",
    "print(\"Shape for X: \" + str(X.shape))\n",
    "print(\"Shape for y: \"+str(y.shape))\n",
    "\n",
    "X= X[762:,:,:,:]\n",
    "y= y[762:]\n",
    "print(\"New shape for X: \" + str(X.shape))\n",
    "print(\"New shape for y: \"+str(y.shape))\n",
    "    \n",
    "dir0= os.path.join(\"FINAL_regression/right_SMA/output\", \"all\")\n",
    "os.mkdir(dir0)\n",
    "dir1=os.path.join(dir0, \"comparison\")\n",
    "dir2=os.path.join(dir0, \"temporal_convolution\")\n",
    "dir3=os.path.join(dir0, \"spatial_convolution\")\n",
    "os.mkdir(dir1)\n",
    "os.mkdir(dir2)\n",
    "os.mkdir(dir3)\n",
    "\n",
    "################################################################################################################################################################################################################################################################################\n",
    "Fs=125\n",
    "EPOCHS=800\n",
    "lr_test=1e-4\n",
    "batch_test=64\n",
    "decay_test=0\n",
    "kern_test=0.15\n",
    "\n",
    "n_folds = 5\n",
    "seed = 21\n",
    "shuffle_test = True\n",
    "r2=[]\n",
    "adj_r2=[]\n",
    "corr_coef=[]\n",
    "final_val_loss=[]\n",
    "stats=[]\n",
    "\n",
    "kfold = KFold(n_splits = n_folds, shuffle = shuffle_test, random_state = seed)\n",
    "count=0\n",
    "sommatoria=0\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    count=count+1\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    print(\"Train index for this split: \"+ str(train_index)) \n",
    "    print(\"Number of samples for train set: \"+str(train_index.shape[0]))\n",
    "    print(\"Test index for this split: \"+ str(test_index))\n",
    "    print(\"Number of samples for test set: \"+str(test_index.shape[0]))\n",
    "\n",
    "    # Define the model architecture - \n",
    "\n",
    "    model=Sequential()\n",
    "\n",
    "    ##################################################################\n",
    "\n",
    "    model.add(Conv2D(8, (1, 64), padding = 'same',\n",
    "                                   input_shape = (N_chan, N_samples_long, 1),\n",
    "                                   use_bias = False, name=\"temporal_conv\"))\n",
    "    model.add(BatchNormalization(name=\"batchnorm_1\"))\n",
    "    model.add(DepthwiseConv2D((31, 1), use_bias = False, \n",
    "                                   depth_multiplier = 2,\n",
    "                                   depthwise_constraint = max_norm(1.), name=\"spatial_conv\"))\n",
    "    model.add(BatchNormalization(name=\"batchnorm_2\"))\n",
    "    model.add(Activation('elu', name=\"activation_1\"))\n",
    "    model.add(AveragePooling2D((1, 4), name=\"pooling_layer_1\"))\n",
    "    model.add(Dropout(0.5, name=\"dropout_1\"))\n",
    "\n",
    "    model.add(SeparableConv2D(16, (1, 16),\n",
    "                                   use_bias = False, padding = 'same', name=\"separable_conv\"))\n",
    "    model.add(BatchNormalization(name=\"batchnorm_3\"))\n",
    "    model.add(Activation('elu', name=\"activation_2\"))\n",
    "    model.add(AveragePooling2D((1, 8), name=\"pooling_layer_2\"))\n",
    "    model.add(Dropout(0.5, name=\"drpout_2\")) #QUI DROPOUT E' LASCIATO A 0.5 come in eegnet paper\n",
    "\n",
    "    model.add(Flatten(name = 'flatten'))\n",
    "\n",
    "    model.add(Dense(NUM_ROI, name = 'dense',\n",
    "                         kernel_constraint = max_norm(kern_test)))\n",
    "#     model.add(Activation('softmax', name = 'softmax'))\n",
    "\n",
    "\n",
    "      # Define the optimizer\n",
    "    optimizer= optimizers.Adam(\n",
    "    learning_rate= lr_test,\n",
    "    weight_decay= decay_test\n",
    "    )\n",
    "    model.compile(optimizer=optimizer,\n",
    "                   loss=loss_fn,\n",
    "                   metrics=[RSquare()])\n",
    "\n",
    "    evaluation.append(model.fit(X_train, y_train, batch_size=batch_test,\n",
    "              epochs=EPOCHS, \n",
    "              validation_data=(X_test, y_test), \n",
    "              verbose=2, workers=1, \n",
    "              callbacks=[CustomCallback()]))\n",
    "\n",
    "    iteration.append(model)\n",
    "    #SALVO R2 e Adjusted R2 score per fold\n",
    "    modello= model.predict(X_test)\n",
    "\n",
    "    temp_r2=[]\n",
    "    temp_adj_r2=[]\n",
    "    temp_r2=r2_score(y_test, modello)\n",
    "\n",
    "    n= y_test.shape[0]\n",
    "    p= N_chan*N_samples_long\n",
    "    temp_adj_r2= 1-((1-temp_r2)*((n-1)/(n-p)))\n",
    "\n",
    "    temp_corr_coef=[]\n",
    "    modello=modello.flatten()\n",
    "    temp_corr_coef=np.corrcoef(y_test, modello)[0,1]\n",
    "\n",
    "    r2.append(temp_r2)\n",
    "    adj_r2.append(temp_adj_r2)\n",
    "    corr_coef.append(temp_corr_coef)\n",
    "\n",
    "    # PLOTTO EFFETTIVA REGRESSIONE VISIVAMENTE\n",
    "    plt.figure()\n",
    "    plt.figure(figsize=(25,5))\n",
    "    plt.plot(y_test)\n",
    "    plt.plot(model.predict(X_test))\n",
    "#     plt.plot(y_test-model.predict(X_test))\n",
    "    plt.savefig(dir1+\"/comparison_kfold_\"+str(count)+\"_zscored_run_clipped\")\n",
    "\n",
    "    var= (model.get_layer(\"temporal_conv\").weights)\n",
    "    for lallo in range(8):\n",
    "        plt.figure()\n",
    "        plt.title(\"temp_conv_\"+str(lallo))\n",
    "        plt.plot(var[0][0,:,0][:,lallo]) #this way i access the temporal filters, cambiando ultimo zero\n",
    "        plt.savefig(dir2+\"/ALL_temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm\")\n",
    "        temp= (var[0][0,:,0][:,lallo]).numpy()\n",
    "        sio.savemat(dir2+\"/ALL_temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm.mat\", {\"array\": temp})\n",
    "        plt.close()\n",
    "\n",
    "        neil_x, neil_y=sp.signal.welch(model.trainable_variables[0].numpy()[0,:,0,lallo], Fs, nperseg=64)\n",
    "        plt.figure()\n",
    "        plt.title(\"Spectrogram_\"+str(lallo))\n",
    "        plt.plot(neil_x, neil_y)\n",
    "        plt.savefig(dir2+\"/ALL_spectrogram_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm\")\n",
    "        plt.close()\n",
    "\n",
    "    var_2= (model.get_layer(\"spatial_conv\").weights)\n",
    "    reshaped_var_2=tf.reshape(var_2[0][:,0,:,:],(31,16))\n",
    "    for lallo in range(16):\n",
    "        nump= reshaped_var_2[:,lallo].numpy()\n",
    "        sio.savemat(dir3+\"/ALL_spat_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm.mat\", {\"array\": nump})\n",
    "\n",
    "# print(\"--------------------------------------- AVERAGE MEAN AND VARIANCE ACROSS FOLDS ---------------------------------------\")\n",
    "# print(np.mean(val_mean_squared_error))\n",
    "# print(np.var(val_mean_squared_error))\n",
    "\n",
    "# PLOTTO LOSS E VALIDATION LOSS, MEAN SQUARED ERROR E VALIDATION MEAN SQUARED ERROR\n",
    "\n",
    "#plot accuracy and loss function across epochs\n",
    "epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "min_temp_loss=10\n",
    "min_temp_acc=10\n",
    "max_temp_loss=0\n",
    "max_temp_acc=0\n",
    "\n",
    "for idx in range(n_folds):\n",
    "    if (np.min(evaluation[idx].history['loss'])<min_temp_loss):\n",
    "        min_temp_loss=np.min(evaluation[idx].history['loss'])\n",
    "    if (np.min(evaluation[idx].history['val_loss'])<min_temp_loss):\n",
    "        min_temp_loss=np.min(evaluation[idx].history['val_loss'])\n",
    "    if (np.max(evaluation[idx].history['loss'])>max_temp_loss):\n",
    "        max_temp_loss=np.max(evaluation[idx].history['loss'])\n",
    "    if (np.max(evaluation[idx].history['val_loss'])>max_temp_loss):\n",
    "        max_temp_loss=np.max(evaluation[idx].history['val_loss'])\n",
    "\n",
    "for idx in range(n_folds):\n",
    "    if (np.min(evaluation[idx].history['r_square'])<min_temp_acc):\n",
    "        min_temp_acc=np.min(evaluation[idx].history['r_square'])\n",
    "    if (np.min(evaluation[idx].history['val_r_square'])<min_temp_acc):\n",
    "        min_temp_acc=np.min(evaluation[idx].history['val_r_square'])\n",
    "    if (np.max(evaluation[idx].history['r_square'])>max_temp_acc):\n",
    "        max_temp_acc=np.max(evaluation[idx].history['r_square'])\n",
    "    if (np.max(evaluation[idx].history['val_r_square'])>max_temp_acc):\n",
    "        max_temp_acc=np.max(evaluation[idx].history['val_r_square'])\n",
    "\n",
    "for idx in range(n_folds):\n",
    "    loss_vec_train= evaluation[idx].history['loss']\n",
    "    loss_vec_test= evaluation[idx].history['val_loss']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "    plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss across epochs for kfold: '+str(idx))\n",
    "    plt.ylim([min_temp_loss, max_temp_loss])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(dir1+\"/loss_kfold_\"+str(idx))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "#plot accuracy and loss function across epochs\n",
    "epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "\n",
    "# inizio=0;\n",
    "# for idx in range(n_folds):\n",
    "#     mse_filter_min= min(MSE_filters_mean)\n",
    "#     mse_filter_max= max(MSE_filters_mean)\n",
    "#     fine=(idx+1)*EPOCHS\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.plot(MSE_filters_mean[inizio:fine],'b-');\n",
    "\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('mean mse filters')\n",
    "#     plt.title('mean_mse_temporal_filters across epochs for kfold: '+str(idx))\n",
    "#     plt.ylim([mse_filter_min, mse_filter_max])\n",
    "# #     plt.legend()\n",
    "\n",
    "#     plt.savefig(dir1+\"/mean_mse_filters_kfold_\"+str(idx))\n",
    "#     plt.close()\n",
    "#     inizio=fine\n",
    "\n",
    "for idx in range(n_folds):\n",
    "    loss_vec_train= evaluation[idx].history['r_square']\n",
    "    loss_vec_test= evaluation[idx].history['val_r_square']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "    plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('r_square')\n",
    "    plt.title('r_square across epochs for kfold: '+str(idx))\n",
    "    plt.ylim([min_temp_acc, max_temp_acc])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(dir1+\"/r_square_kfold_\"+str(idx))\n",
    "    plt.close()\n",
    "\n",
    "for idx in range(n_folds):\n",
    "    temp_val_loss=[]\n",
    "    temp_val_loss= evaluation[idx].history['val_loss'][-1] #with this indexing i select the last element of the list\n",
    "    final_val_loss.append(temp_val_loss)\n",
    "\n",
    "gianfranco= model.predict(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(y, 'b', label='true')\n",
    "plt.plot(gianfranco, 'r', label='predicted')\n",
    "plt.legend()\n",
    "plt.savefig(dir1+\"/total_comparison_zscored_run_clipped\")\n",
    "plt.savefig(dir0+\"/total_comparison_zscored_run_clipped\")\n",
    "\n",
    "sio.savemat(dir1+\"/regression_results_zscored_run_clipped.mat\", {\"array\": gianfranco})\n",
    "\n",
    "sio.savemat(dir1+\"/r2_score_zscored_run_clipped.mat\", {\"array\": r2})\n",
    "sio.savemat(dir1+\"/adjusted_r2_score_zscored_run_clipped.mat\", {\"array\": adj_r2})\n",
    "sio.savemat(dir1+\"/corr_coef_zscored_run_clipped.mat\", {\"array\": corr_coef})\n",
    "sio.savemat(dir1+\"/final_val_loss_zscored_run_clipped.mat\", {\"array\": final_val_loss})\n",
    "\n",
    "avg_final_loss= np.mean(final_val_loss)\n",
    "var_final_loss=statistics.pstdev(final_val_loss)\n",
    "avg_r2= np.mean(r2)\n",
    "var_r2=statistics.pstdev(r2)\n",
    "avg_corr_coef= np.mean(corr_coef)\n",
    "var_corr_coef= statistics.pstdev(corr_coef)\n",
    "\n",
    "stats.append(avg_final_loss)\n",
    "stats.append(var_final_loss)\n",
    "stats.append(avg_r2)\n",
    "stats.append(var_r2)\n",
    "stats.append(avg_corr_coef)\n",
    "stats.append(var_corr_coef)\n",
    "sio.savemat(dir0+\"/stats_zscored_run_clipped.mat\", {\"array\": stats})\n",
    "\n",
    "####################################################################################################################################\n",
    "\n",
    "mse_epoca=[]\n",
    "for epoca in range(EPOCHS*n_folds):\n",
    "    mse_filtro=[]\n",
    "    for filtro in range(8):\n",
    "        curr= lista[epoca][filtro][:]\n",
    "        precedente= lista[epoca-1][filtro][:]    \n",
    "        mse_filtro.append(((curr-precedente)**2).mean(axis=None))\n",
    "\n",
    "    mse_epoca.append(np.mean(mse_filtro))    \n",
    "\n",
    "inizio=0\n",
    "for idx in range(n_folds):\n",
    "    mse_filter_min= min(mse_epoca)\n",
    "    mse_filter_max= max(mse_epoca)\n",
    "    fine=(idx+1)*EPOCHS\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(mse_epoca[inizio+1:fine],'b-');\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('mean mse filters')\n",
    "    plt.title('mean_mse_temporal_filters across epochs for kfold: '+str(idx))\n",
    "#     plt.ylim([mse_filter_min, mse_filter_max])\n",
    "    plt.legend()\n",
    "    sio.savemat(dir1+\"/mean_mse_filter_kfold_\"+str(idx)+\".mat\", {\"array\": mse_epoca[inizio+1:fine]})\n",
    "\n",
    "    plt.savefig(dir1+\"/mean_mse_filters_kfold_\"+str(idx))\n",
    "    plt.close()\n",
    "    inizio=fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab18bf",
   "metadata": {},
   "source": [
    "# forced filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9aebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs=125\n",
    "b,a=sp.signal.butter(10, Wn=30, btype=\"lowpass\", fs=Fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa4ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "            new_weights=[]\n",
    "            MSE_filters=[]\n",
    "            for idx in range(8):\n",
    "                old_weights= (model.trainable_variables[0].numpy()[0,:,0,idx])\n",
    "                curr_new_weights=sp.signal.lfilter(b,a,old_weights)\n",
    "\n",
    "                new_weights.append(curr_new_weights)\n",
    "                MSE_filters.append( ((curr_new_weights-old_weights)**2).mean(axis=None) )\n",
    "\n",
    "\n",
    "            new_weights=np.stack(new_weights)\n",
    "            new_weights=np.transpose(new_weights)\n",
    "            new_weights=new_weights.reshape(1,64,1,8)\n",
    "            self.model.trainable_variables[0].assign(tf.Variable(new_weights, dtype=tf.float32))\n",
    "\n",
    "            MSE_filters_mean.append(np.mean(MSE_filters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SINGLE ROI\n",
    "SUBJECTS=[1,2,3,4,5,6,7,8,9,11,12,13,14,16,17]\n",
    "for temp_sub in SUBJECTS:\n",
    "    lista=[]\n",
    "    sub=\"{:02d}\".format(temp_sub)\n",
    "\n",
    "    #Define loss function\n",
    "    loss_fn= tf.keras.losses.MeanSquaredError()\n",
    "    evaluation=[]\n",
    "    iteration=[]\n",
    "    confusion_matrix_x_test=[]\n",
    "    confusion_matrix_y_test= []\n",
    "    validation_acc=[]\n",
    "    MSE_filters_mean=[]\n",
    "    NUM_ROI=1\n",
    "\n",
    "    print(\"SUBJECT: \"+ str(sub))\n",
    "    \n",
    "    X=np.load(\"input/sub_\"+str(sub)+ \"_compiled_RAW_downsampled_float32.npy\")\n",
    "    y= sio.loadmat(\"FINAL_regression/right_SMA/input/sub-\"+str(sub)+\"_compiled_bold_non_norm_z_score_run_clipped.mat\")[\"compiled_bold_non_norm_z_score_run_clipped\"]\n",
    "\n",
    "    X= X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "    y=y.flatten()\n",
    "    print(\"New shape for X: \" + str(X.shape))\n",
    "    print(\"New shape for y: \"+str(y.shape))\n",
    "\n",
    "    dir0= os.path.join(\"FINAL_regression/right_SMA/output\", \"forced_filtering_sub_\"+str(sub))\n",
    "    os.mkdir(dir0)\n",
    "    dir1=os.path.join(dir0, \"comparison\")\n",
    "    dir2=os.path.join(dir0, \"temporal_convolution\")\n",
    "    dir3=os.path.join(dir0, \"spatial_convolution\")\n",
    "    os.mkdir(dir1)\n",
    "    os.mkdir(dir2)\n",
    "    os.mkdir(dir3)\n",
    "\n",
    "    ################################################################################################################################################################################################################################################################################\n",
    "    Fs=125\n",
    "    EPOCHS=800\n",
    "    lr_test=1e-4\n",
    "    batch_test=64\n",
    "    decay_test=0\n",
    "    kern_test=0.15\n",
    "\n",
    "    n_folds = 5\n",
    "    seed = 21\n",
    "    shuffle_test = True\n",
    "    r2=[]\n",
    "    adj_r2=[]\n",
    "    corr_coef=[]\n",
    "    final_val_loss=[]\n",
    "    stats=[]\n",
    "\n",
    "    kfold = KFold(n_splits = n_folds, shuffle = shuffle_test, random_state = seed)\n",
    "    count=0\n",
    "    sommatoria=0\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        count=count+1\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        print(\"Train index for this split: \"+ str(train_index)) \n",
    "        print(\"Number of samples for train set: \"+str(train_index.shape[0]))\n",
    "        print(\"Test index for this split: \"+ str(test_index))\n",
    "        print(\"Number of samples for test set: \"+str(test_index.shape[0]))\n",
    "\n",
    "        # Define the model architecture - \n",
    "\n",
    "        model=Sequential()\n",
    "\n",
    "        ##################################################################\n",
    "\n",
    "        model.add(Conv2D(8, (1, 64), padding = 'same',\n",
    "                                       input_shape = (N_chan, N_samples_long, 1),\n",
    "                                       use_bias = False, name=\"temporal_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_1\"))\n",
    "        model.add(DepthwiseConv2D((31, 1), use_bias = False, \n",
    "                                       depth_multiplier = 2,\n",
    "                                       depthwise_constraint = max_norm(1.), name=\"spatial_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_2\"))\n",
    "        model.add(Activation('elu', name=\"activation_1\"))\n",
    "        model.add(AveragePooling2D((1, 4), name=\"pooling_layer_1\"))\n",
    "        model.add(Dropout(0.5, name=\"dropout_1\"))\n",
    "\n",
    "        model.add(SeparableConv2D(16, (1, 16),\n",
    "                                       use_bias = False, padding = 'same', name=\"separable_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_3\"))\n",
    "        model.add(Activation('elu', name=\"activation_2\"))\n",
    "        model.add(AveragePooling2D((1, 8), name=\"pooling_layer_2\"))\n",
    "        model.add(Dropout(0.5, name=\"drpout_2\"))\n",
    "\n",
    "        model.add(Flatten(name = 'flatten'))\n",
    "\n",
    "        model.add(Dense(1, name = 'dense',\n",
    "                                 kernel_constraint = max_norm(kern_test)))\n",
    "    #     model.add(Activation('softmax', name = 'softmax'))\n",
    "\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer= optimizers.Adam(\n",
    "        learning_rate= lr_test,\n",
    "        weight_decay= decay_test\n",
    "        )\n",
    "        model.compile(optimizer=optimizer,\n",
    "                       loss=loss_fn,\n",
    "                       metrics=[RSquare()])\n",
    "\n",
    "        evaluation.append(model.fit(X_train, y_train, batch_size=batch_test,\n",
    "                  epochs=EPOCHS, \n",
    "                  validation_data=(X_test, y_test), \n",
    "                  verbose=2, workers=1, \n",
    "                  callbacks=[CustomCallback()]))\n",
    "\n",
    "        iteration.append(model)\n",
    "        #SALVO R2 e Adjusted R2 score per fold\n",
    "        modello= model.predict(X_test)\n",
    "\n",
    "        temp_r2=[]\n",
    "        temp_adj_r2=[]\n",
    "        temp_r2=r2_score(y_test, modello)\n",
    "\n",
    "        n= y_test.shape[0]\n",
    "        p= N_chan*N_samples_long\n",
    "        temp_adj_r2= 1-((1-temp_r2)*((n-1)/(n-p)))\n",
    "\n",
    "        temp_corr_coef=[]\n",
    "        modello=modello.flatten()\n",
    "        temp_corr_coef=np.corrcoef(y_test, modello)[0,1]\n",
    "\n",
    "        r2.append(temp_r2)\n",
    "        adj_r2.append(temp_adj_r2)\n",
    "        corr_coef.append(temp_corr_coef)\n",
    "\n",
    "        # PLOTTO EFFETTIVA REGRESSIONE VISIVAMENTE\n",
    "        plt.figure()\n",
    "        plt.figure(figsize=(25,5))\n",
    "        plt.plot(y_test)\n",
    "        plt.plot(model.predict(X_test))\n",
    "    #     plt.plot(y_test-model.predict(X_test))\n",
    "        plt.savefig(dir1+\"/comparison_kfold_\"+str(count)+\"_zscored_run_clipped\")\n",
    "\n",
    "        var= (model.get_layer(\"temporal_conv\").weights)\n",
    "        for lallo in range(8):\n",
    "            plt.figure()\n",
    "            plt.title(\"temp_conv_\"+str(lallo))\n",
    "            plt.plot(var[0][0,:,0][:,lallo]) #this way i access the temporal filters, cambiando ultimo zero\n",
    "            plt.savefig(dir2+\"/ALL_temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm\")\n",
    "            temp= (var[0][0,:,0][:,lallo]).numpy()\n",
    "            sio.savemat(dir2+\"/ALL_temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm.mat\", {\"array\": temp})\n",
    "            plt.close()\n",
    "\n",
    "            neil_x, neil_y=sp.signal.welch(model.trainable_variables[0].numpy()[0,:,0,lallo], Fs, nperseg=64)\n",
    "            plt.figure()\n",
    "            plt.title(\"Spectrogram_\"+str(lallo))\n",
    "            plt.plot(neil_x, neil_y)\n",
    "            plt.savefig(dir2+\"/ALL_spectrogram_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm\")\n",
    "            plt.close()\n",
    "\n",
    "        var_2= (model.get_layer(\"spatial_conv\").weights)\n",
    "        reshaped_var_2=tf.reshape(var_2[0][:,0,:,:],(31,16))\n",
    "        for lallo in range(16):\n",
    "            nump= reshaped_var_2[:,lallo].numpy()\n",
    "            sio.savemat(dir3+\"/ALL_spat_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm.mat\", {\"array\": nump})\n",
    "\n",
    "    # print(\"--------------------------------------- AVERAGE MEAN AND VARIANCE ACROSS FOLDS ---------------------------------------\")\n",
    "    # print(np.mean(val_mean_squared_error))\n",
    "    # print(np.var(val_mean_squared_error))\n",
    "\n",
    "    # PLOTTO LOSS E VALIDATION LOSS, MEAN SQUARED ERROR E VALIDATION MEAN SQUARED ERROR\n",
    "\n",
    "    #plot accuracy and loss function across epochs\n",
    "    epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "    min_temp_loss=10\n",
    "    min_temp_acc=10\n",
    "    max_temp_loss=0\n",
    "    max_temp_acc=0\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        if (np.min(evaluation[idx].history['loss'])<min_temp_loss):\n",
    "            min_temp_loss=np.min(evaluation[idx].history['loss'])\n",
    "        if (np.min(evaluation[idx].history['val_loss'])<min_temp_loss):\n",
    "            min_temp_loss=np.min(evaluation[idx].history['val_loss'])\n",
    "        if (np.max(evaluation[idx].history['loss'])>max_temp_loss):\n",
    "            max_temp_loss=np.max(evaluation[idx].history['loss'])\n",
    "        if (np.max(evaluation[idx].history['val_loss'])>max_temp_loss):\n",
    "            max_temp_loss=np.max(evaluation[idx].history['val_loss'])\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        if (np.min(evaluation[idx].history['r_square'])<min_temp_acc):\n",
    "            min_temp_acc=np.min(evaluation[idx].history['r_square'])\n",
    "        if (np.min(evaluation[idx].history['val_r_square'])<min_temp_acc):\n",
    "            min_temp_acc=np.min(evaluation[idx].history['val_r_square'])\n",
    "        if (np.max(evaluation[idx].history['r_square'])>max_temp_acc):\n",
    "            max_temp_acc=np.max(evaluation[idx].history['r_square'])\n",
    "        if (np.max(evaluation[idx].history['val_r_square'])>max_temp_acc):\n",
    "            max_temp_acc=np.max(evaluation[idx].history['val_r_square'])\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        loss_vec_train= evaluation[idx].history['loss']\n",
    "        loss_vec_test= evaluation[idx].history['val_loss']\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "        plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss across epochs for kfold: '+str(idx))\n",
    "        plt.ylim([min_temp_loss, max_temp_loss])\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(dir1+\"/loss_kfold_\"+str(idx))\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    #plot accuracy and loss function across epochs\n",
    "    epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "\n",
    "    inizio=0;\n",
    "    for idx in range(n_folds):\n",
    "        mse_filter_min= min(MSE_filters_mean)\n",
    "        mse_filter_max= max(MSE_filters_mean)\n",
    "        fine=(idx+1)*EPOCHS\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(MSE_filters_mean[inizio:fine],'b-');\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('mean mse filters')\n",
    "        plt.title('mean_mse_temporal_filters across epochs for kfold: '+str(idx))\n",
    "        plt.ylim([mse_filter_min, mse_filter_max])\n",
    "#         plt.legend()\n",
    "        sio.savemat(dir1+\"/mean_mse_filter_kfold_\"+str(idx)+\".mat\", {\"array\": MSE_filters_mean[inizio:fine]})\n",
    "\n",
    "        plt.savefig(dir1+\"/mean_mse_filters_kfold_\"+str(idx))\n",
    "        plt.close()\n",
    "        inizio=fine\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        loss_vec_train= evaluation[idx].history['r_square']\n",
    "        loss_vec_test= evaluation[idx].history['val_r_square']\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "        plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('r_square')\n",
    "        plt.title('r_square across epochs for kfold: '+str(idx))\n",
    "        plt.ylim([min_temp_acc, max_temp_acc])\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(dir1+\"/r_square_kfold_\"+str(idx))\n",
    "        plt.close()\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        temp_val_loss=[]\n",
    "        temp_val_loss= evaluation[idx].history['val_loss'][-1] #with this indexing i select the last element of the list\n",
    "        final_val_loss.append(temp_val_loss)\n",
    "\n",
    "    gianfranco= model.predict(X)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.figure(figsize=(25,5))\n",
    "    plt.plot(y, 'b', label='true')\n",
    "    plt.plot(gianfranco, 'r', label='predicted')\n",
    "    plt.legend()\n",
    "    plt.savefig(dir1+\"/total_comparison_zscored_run_clipped\")\n",
    "    plt.savefig(dir0+\"/total_comparison_zscored_run_clipped\")\n",
    "\n",
    "    sio.savemat(dir1+\"/regression_results_zscored_run_clipped.mat\", {\"array\": gianfranco})\n",
    "\n",
    "    sio.savemat(dir1+\"/r2_score_zscored_run_clipped.mat\", {\"array\": r2})\n",
    "    sio.savemat(dir1+\"/adjusted_r2_score_zscored_run_clipped.mat\", {\"array\": adj_r2})\n",
    "    sio.savemat(dir1+\"/corr_coef_zscored_run_clipped.mat\", {\"array\": corr_coef})\n",
    "    sio.savemat(dir1+\"/final_val_loss_zscored_run_clipped.mat\", {\"array\": final_val_loss})\n",
    "\n",
    "    avg_final_loss= np.mean(final_val_loss)\n",
    "    var_final_loss=statistics.pstdev(final_val_loss)\n",
    "    avg_r2= np.mean(r2)\n",
    "    var_r2=statistics.pstdev(r2)\n",
    "    avg_corr_coef= np.mean(corr_coef)\n",
    "    var_corr_coef= statistics.pstdev(corr_coef)\n",
    "    \n",
    "    stats.append(avg_final_loss)\n",
    "    stats.append(var_final_loss)\n",
    "    stats.append(avg_r2)\n",
    "    stats.append(var_r2)\n",
    "    stats.append(avg_corr_coef)\n",
    "    stats.append(var_corr_coef)\n",
    "    sio.savemat(dir0+\"/stats_zscored_run_clipped.mat\", {\"array\": stats})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1875b377",
   "metadata": {},
   "source": [
    "# all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaf6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista=[]\n",
    "\n",
    "#Define loss function\n",
    "loss_fn= tf.keras.losses.MeanSquaredError()\n",
    "evaluation=[]\n",
    "iteration=[]\n",
    "confusion_matrix_x_test=[]\n",
    "confusion_matrix_y_test= []\n",
    "validation_acc=[]\n",
    "MSE_filters_mean=[]\n",
    "NUM_ROI=1\n",
    "\n",
    "X=np.load(\"input/RAW_regression_all_subs_float32.npy\")\n",
    "y= sio.loadmat(\"FINAL_regression/right_SMA/input/compiled_bold_non_norm_all_z_score_run_clipped.mat\")[\"compiled_bold_non_norm_all_z_score_run_clipped\"]\n",
    "\n",
    "X= X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "y=y.flatten()\n",
    "print(\"Shape for X: \" + str(X.shape))\n",
    "print(\"Shape for y: \"+str(y.shape))\n",
    "\n",
    "X= X[762:,:,:,:]\n",
    "y= y[762:]\n",
    "print(\"New shape for X: \" + str(X.shape))\n",
    "print(\"New shape for y: \"+str(y.shape))\n",
    "    \n",
    "dir0= os.path.join(\"FINAL_regression/right_SMA/output\", \"forced_filtering_all\")\n",
    "os.mkdir(dir0)\n",
    "dir1=os.path.join(dir0, \"comparison\")\n",
    "dir2=os.path.join(dir0, \"temporal_convolution\")\n",
    "dir3=os.path.join(dir0, \"spatial_convolution\")\n",
    "os.mkdir(dir1)\n",
    "os.mkdir(dir2)\n",
    "os.mkdir(dir3)\n",
    "\n",
    "################################################################################################################################################################################################################################################################################\n",
    "Fs=125\n",
    "EPOCHS=800\n",
    "lr_test=1e-4\n",
    "batch_test=64\n",
    "decay_test=0\n",
    "kern_test=0.15\n",
    "\n",
    "n_folds = 5\n",
    "seed = 21\n",
    "shuffle_test = True\n",
    "r2=[]\n",
    "adj_r2=[]\n",
    "corr_coef=[]\n",
    "final_val_loss=[]\n",
    "stats=[]\n",
    "\n",
    "kfold = KFold(n_splits = n_folds, shuffle = shuffle_test, random_state = seed)\n",
    "count=0\n",
    "sommatoria=0\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    count=count+1\n",
    "\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    print(\"Train index for this split: \"+ str(train_index)) \n",
    "    print(\"Number of samples for train set: \"+str(train_index.shape[0]))\n",
    "    print(\"Test index for this split: \"+ str(test_index))\n",
    "    print(\"Number of samples for test set: \"+str(test_index.shape[0]))\n",
    "\n",
    "    # Define the model architecture - \n",
    "\n",
    "    model=Sequential()\n",
    "\n",
    "    ##################################################################\n",
    "\n",
    "    model.add(Conv2D(8, (1, 64), padding = 'same',\n",
    "                                   input_shape = (N_chan, N_samples_long, 1),\n",
    "                                   use_bias = False, name=\"temporal_conv\"))\n",
    "    model.add(BatchNormalization(name=\"batchnorm_1\"))\n",
    "    model.add(DepthwiseConv2D((31, 1), use_bias = False, \n",
    "                                   depth_multiplier = 2,\n",
    "                                   depthwise_constraint = max_norm(1.), name=\"spatial_conv\"))\n",
    "    model.add(BatchNormalization(name=\"batchnorm_2\"))\n",
    "    model.add(Activation('elu', name=\"activation_1\"))\n",
    "    model.add(AveragePooling2D((1, 4), name=\"pooling_layer_1\"))\n",
    "    model.add(Dropout(0.5, name=\"dropout_1\"))\n",
    "\n",
    "    model.add(SeparableConv2D(16, (1, 16),\n",
    "                                   use_bias = False, padding = 'same', name=\"separable_conv\"))\n",
    "    model.add(BatchNormalization(name=\"batchnorm_3\"))\n",
    "    model.add(Activation('elu', name=\"activation_2\"))\n",
    "    model.add(AveragePooling2D((1, 8), name=\"pooling_layer_2\"))\n",
    "    model.add(Dropout(0.5, name=\"drpout_2\")) #QUI DROPOUT E' LASCIATO A 0.5 come in eegnet paper\n",
    "\n",
    "    model.add(Flatten(name = 'flatten'))\n",
    "\n",
    "    model.add(Dense(NUM_ROI, name = 'dense',\n",
    "                         kernel_constraint = max_norm(kern_test)))\n",
    "#     model.add(Activation('softmax', name = 'softmax'))\n",
    "\n",
    "\n",
    "      # Define the optimizer\n",
    "    optimizer= optimizers.Adam(\n",
    "    learning_rate= lr_test,\n",
    "    weight_decay= decay_test\n",
    "    )\n",
    "    model.compile(optimizer=optimizer,\n",
    "                   loss=loss_fn,\n",
    "                   metrics=[RSquare()])\n",
    "\n",
    "    evaluation.append(model.fit(X_train, y_train, batch_size=batch_test,\n",
    "              epochs=EPOCHS, \n",
    "              validation_data=(X_test, y_test), \n",
    "              verbose=2, workers=1, \n",
    "              callbacks=[CustomCallback()]))\n",
    "\n",
    "    iteration.append(model)\n",
    "    #SALVO R2 e Adjusted R2 score per fold\n",
    "    modello= model.predict(X_test)\n",
    "\n",
    "    temp_r2=[]\n",
    "    temp_adj_r2=[]\n",
    "    temp_r2=r2_score(y_test, modello)\n",
    "\n",
    "    n= y_test.shape[0]\n",
    "    p= N_chan*N_samples_long\n",
    "    temp_adj_r2= 1-((1-temp_r2)*((n-1)/(n-p)))\n",
    "\n",
    "    temp_corr_coef=[]\n",
    "    modello=modello.flatten()\n",
    "    temp_corr_coef=np.corrcoef(y_test, modello)[0,1]\n",
    "\n",
    "    r2.append(temp_r2)\n",
    "    adj_r2.append(temp_adj_r2)\n",
    "    corr_coef.append(temp_corr_coef)\n",
    "\n",
    "    # PLOTTO EFFETTIVA REGRESSIONE VISIVAMENTE\n",
    "    plt.figure()\n",
    "    plt.figure(figsize=(25,5))\n",
    "    plt.plot(y_test)\n",
    "    plt.plot(model.predict(X_test))\n",
    "#     plt.plot(y_test-model.predict(X_test))\n",
    "    plt.savefig(dir1+\"/comparison_kfold_\"+str(count)+\"_zscored_run_clipped\")\n",
    "\n",
    "    var= (model.get_layer(\"temporal_conv\").weights)\n",
    "    for lallo in range(8):\n",
    "        plt.figure()\n",
    "        plt.title(\"temp_conv_\"+str(lallo))\n",
    "        plt.plot(var[0][0,:,0][:,lallo]) #this way i access the temporal filters, cambiando ultimo zero\n",
    "        plt.savefig(dir2+\"/ALL_temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm\")\n",
    "        temp= (var[0][0,:,0][:,lallo]).numpy()\n",
    "        sio.savemat(dir2+\"/ALL_temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm.mat\", {\"array\": temp})\n",
    "        plt.close()\n",
    "\n",
    "        neil_x, neil_y=sp.signal.welch(model.trainable_variables[0].numpy()[0,:,0,lallo], Fs, nperseg=64)\n",
    "        plt.figure()\n",
    "        plt.title(\"Spectrogram_\"+str(lallo))\n",
    "        plt.plot(neil_x, neil_y)\n",
    "        plt.savefig(dir2+\"/ALL_spectrogram_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm\")\n",
    "        plt.close()\n",
    "\n",
    "    var_2= (model.get_layer(\"spatial_conv\").weights)\n",
    "    reshaped_var_2=tf.reshape(var_2[0][:,0,:,:],(31,16))\n",
    "    for lallo in range(16):\n",
    "        nump= reshaped_var_2[:,lallo].numpy()\n",
    "        sio.savemat(dir3+\"/ALL_spat_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo)+\"_non_norm.mat\", {\"array\": nump})\n",
    "\n",
    "# print(\"--------------------------------------- AVERAGE MEAN AND VARIANCE ACROSS FOLDS ---------------------------------------\")\n",
    "# print(np.mean(val_mean_squared_error))\n",
    "# print(np.var(val_mean_squared_error))\n",
    "\n",
    "# PLOTTO LOSS E VALIDATION LOSS, MEAN SQUARED ERROR E VALIDATION MEAN SQUARED ERROR\n",
    "\n",
    "#plot accuracy and loss function across epochs\n",
    "epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "min_temp_loss=10\n",
    "min_temp_acc=10\n",
    "max_temp_loss=0\n",
    "max_temp_acc=0\n",
    "\n",
    "for idx in range(n_folds):\n",
    "    if (np.min(evaluation[idx].history['loss'])<min_temp_loss):\n",
    "        min_temp_loss=np.min(evaluation[idx].history['loss'])\n",
    "    if (np.min(evaluation[idx].history['val_loss'])<min_temp_loss):\n",
    "        min_temp_loss=np.min(evaluation[idx].history['val_loss'])\n",
    "    if (np.max(evaluation[idx].history['loss'])>max_temp_loss):\n",
    "        max_temp_loss=np.max(evaluation[idx].history['loss'])\n",
    "    if (np.max(evaluation[idx].history['val_loss'])>max_temp_loss):\n",
    "        max_temp_loss=np.max(evaluation[idx].history['val_loss'])\n",
    "\n",
    "for idx in range(n_folds):\n",
    "    if (np.min(evaluation[idx].history['r_square'])<min_temp_acc):\n",
    "        min_temp_acc=np.min(evaluation[idx].history['r_square'])\n",
    "    if (np.min(evaluation[idx].history['val_r_square'])<min_temp_acc):\n",
    "        min_temp_acc=np.min(evaluation[idx].history['val_r_square'])\n",
    "    if (np.max(evaluation[idx].history['r_square'])>max_temp_acc):\n",
    "        max_temp_acc=np.max(evaluation[idx].history['r_square'])\n",
    "    if (np.max(evaluation[idx].history['val_r_square'])>max_temp_acc):\n",
    "        max_temp_acc=np.max(evaluation[idx].history['val_r_square'])\n",
    "\n",
    "for idx in range(n_folds):\n",
    "    loss_vec_train= evaluation[idx].history['loss']\n",
    "    loss_vec_test= evaluation[idx].history['val_loss']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "    plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss across epochs for kfold: '+str(idx))\n",
    "    plt.ylim([min_temp_loss, max_temp_loss])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(dir1+\"/loss_kfold_\"+str(idx))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "#plot accuracy and loss function across epochs\n",
    "epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "\n",
    "inizio=0;\n",
    "for idx in range(n_folds):\n",
    "    mse_filter_min= min(MSE_filters_mean)\n",
    "    mse_filter_max= max(MSE_filters_mean)\n",
    "    fine=(idx+1)*EPOCHS\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(MSE_filters_mean[inizio:fine],'b-');\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('mean mse filters')\n",
    "    plt.title('mean_mse_temporal_filters across epochs for kfold: '+str(idx))\n",
    "    plt.ylim([mse_filter_min, mse_filter_max])\n",
    "#     plt.legend()\n",
    "    sio.savemat(dir1+\"/mean_mse_filter_kfold_\"+str(idx)+\".mat\", {\"array\": MSE_filters_mean[inizio:fine]})\n",
    "\n",
    "    plt.savefig(dir1+\"/mean_mse_filters_kfold_\"+str(idx))\n",
    "    plt.close()\n",
    "    inizio=fine\n",
    "\n",
    "for idx in range(n_folds):\n",
    "    loss_vec_train= evaluation[idx].history['r_square']\n",
    "    loss_vec_test= evaluation[idx].history['val_r_square']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "    plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('r_square')\n",
    "    plt.title('r_square across epochs for kfold: '+str(idx))\n",
    "    plt.ylim([min_temp_acc, max_temp_acc])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(dir1+\"/r_square_kfold_\"+str(idx))\n",
    "    plt.close()\n",
    "\n",
    "for idx in range(n_folds):\n",
    "    temp_val_loss=[]\n",
    "    temp_val_loss= evaluation[idx].history['val_loss'][-1] #with this indexing i select the last element of the list\n",
    "    final_val_loss.append(temp_val_loss)\n",
    "\n",
    "gianfranco= model.predict(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(y, 'b', label='true')\n",
    "plt.plot(gianfranco, 'r', label='predicted')\n",
    "plt.legend()\n",
    "plt.savefig(dir1+\"/total_comparison_zscored_run_clipped\")\n",
    "plt.savefig(dir0+\"/total_comparison_zscored_run_clipped\")\n",
    "\n",
    "sio.savemat(dir1+\"/regression_results_zscored_run_clipped.mat\", {\"array\": gianfranco})\n",
    "\n",
    "sio.savemat(dir1+\"/r2_score_zscored_run_clipped.mat\", {\"array\": r2})\n",
    "sio.savemat(dir1+\"/adjusted_r2_score_zscored_run_clipped.mat\", {\"array\": adj_r2})\n",
    "sio.savemat(dir1+\"/corr_coef_zscored_run_clipped.mat\", {\"array\": corr_coef})\n",
    "sio.savemat(dir1+\"/final_val_loss_zscored_run_clipped.mat\", {\"array\": final_val_loss})\n",
    "\n",
    "avg_final_loss= np.mean(final_val_loss)\n",
    "var_final_loss=statistics.pstdev(final_val_loss)\n",
    "avg_r2= np.mean(r2)\n",
    "var_r2=statistics.pstdev(r2)\n",
    "avg_corr_coef= np.mean(corr_coef)\n",
    "var_corr_coef= statistics.pstdev(corr_coef)\n",
    "\n",
    "stats.append(avg_final_loss)\n",
    "stats.append(var_final_loss)\n",
    "stats.append(avg_r2)\n",
    "stats.append(var_r2)\n",
    "stats.append(avg_corr_coef)\n",
    "stats.append(var_corr_coef)\n",
    "sio.savemat(dir0+\"/stats_zscored_run_clipped.mat\", {\"array\": stats})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
