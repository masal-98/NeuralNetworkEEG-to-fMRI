{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be306ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import SpatialDropout2D\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.layers import Input, Flatten\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras import backend as K\n",
    "import keras\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.losses as losses\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics \n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "\n",
    "from tensorflow_addons.metrics import RSquare\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from wandb.keras import WandbCallback\n",
    "import wandb\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968770ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_test=8\n",
    "D_test=2\n",
    "F2_test=16\n",
    "Drop_test=0 #originally 0.5\n",
    "KernLength_test=64 #Originally 64 \n",
    "batch_test=16\n",
    "\n",
    "N_chan=31\n",
    "N_samples_long=250\n",
    "# N_samples_short=251\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER SUB \n",
    "# ########SUBJECTS=[1,2,3,4,5,6,7,8,9,11,12,13,14,16,17]\n",
    "SUBJECTS=[9]\n",
    "for temp_sub in SUBJECTS:\n",
    "    CLASSES= [50]\n",
    "    #questa va applicata a for loop di subject che deve essere il pi첫 esterno \n",
    "    sub=\"{:02d}\".format(temp_sub)\n",
    "\n",
    "    dir00= os.path.join(\"FINAL_fuzzy_regressor/MinMax_per_sub/output\", \"sub_\"+str(sub))\n",
    "#     os.mkdir(dir00)\n",
    "    for n_classes in CLASSES:\n",
    "        #Define loss function\n",
    "        loss_fn= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "        LABELS= list(range(0,n_classes))\n",
    "        \n",
    "        if n_classes==50:\n",
    "            numero_classi=5\n",
    "        else:\n",
    "            numero_classi=n_classes\n",
    "        \n",
    "        evaluation=[]\n",
    "        iteration=[]\n",
    "        confusion_matrix_x_test=[]\n",
    "        confusion_matrix_y_test= []\n",
    "        validation_acc=[]\n",
    "        PERFORMANCE=[]\n",
    "\n",
    "        print(\"SUBJECT: \"+ str(sub))\n",
    "        print(\"N_CLASSES: \"+ str(n_classes))\n",
    "        \n",
    "        X=np.load(\"input/sub_\"+str(sub)+ \"_compiled_RAW_downsampled_float32.npy\")\n",
    "        y= sio.loadmat(\"FINAL_fuzzy_regressor/MinMax_per_sub/input/sub-\"+str(sub)+\"_compiled_rightSMA_scaled_per_sub_\"+str(n_classes)+\"class_balanced.mat\")[\"compiled_rightSMA_labels\"]\n",
    "\n",
    "        X= X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "        y=y.flatten()\n",
    "        print(\"New shape for X: \" + str(X.shape))\n",
    "        print(\"New shape for y: \"+str(y.shape))\n",
    "\n",
    "        dir0= os.path.join(dir00, str(n_classes)+\"_class\")\n",
    "#         os.mkdir(dir0)\n",
    "        dir1=os.path.join(dir0, \"comparison\")\n",
    "        dir2=os.path.join(dir0, \"temporal_convolution\")\n",
    "        dir3=os.path.join(dir0, \"spatial_convolution\")\n",
    "#         os.mkdir(dir1)\n",
    "#         os.mkdir(dir2)\n",
    "#         os.mkdir(dir3)\n",
    "\n",
    "        ################################################################################################################################################################################################################################################################################\n",
    "\n",
    "        n_folds = 5\n",
    "        seed = 21\n",
    "        shuffle_test = True\n",
    "        EPOCHS=250\n",
    "\n",
    "        kfold = KFold(n_splits = n_folds, shuffle = shuffle_test, random_state = seed)\n",
    "        count=0\n",
    "        sommatoria=0\n",
    "\n",
    "        for train_index, test_index in kfold.split(X):\n",
    "            count=count+1\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            print(\"Train index for this split: \"+ str(train_index)) \n",
    "            print(\"Number of samples for train set: \"+str(train_index.shape[0]))\n",
    "            print(\"Test index for this split: \"+ str(test_index))\n",
    "            print(\"Number of samples for test set: \"+str(test_index.shape[0]))\n",
    "\n",
    "            # Define the model architecture - \n",
    "\n",
    "            model=Sequential()\n",
    "\n",
    "            ##################################################################\n",
    "\n",
    "            model.add(Conv2D(8, (1, 64), padding = 'same',\n",
    "                                           input_shape = (N_chan, N_samples_long, 1),\n",
    "                                           use_bias = False, name=\"temporal_conv\"))\n",
    "            model.add(BatchNormalization(name=\"batchnorm_1\"))\n",
    "            model.add(DepthwiseConv2D((31, 1), use_bias = False, \n",
    "                                           depth_multiplier = 2,\n",
    "                                           depthwise_constraint = max_norm(1.), name=\"spatial_conv\"))\n",
    "            model.add(BatchNormalization(name=\"batchnorm_2\"))\n",
    "            model.add(Activation('elu', name=\"activation_1\"))\n",
    "            model.add(AveragePooling2D((1, 4), name=\"pooling_layer_1\"))\n",
    "            model.add(Dropout(0.5, name=\"dropout_1\"))\n",
    "\n",
    "            model.add(SeparableConv2D(16, (1, 16),\n",
    "                                           use_bias = False, padding = 'same', name=\"separable_conv\"))\n",
    "            model.add(BatchNormalization(name=\"batchnorm_3\"))\n",
    "            model.add(Activation('elu', name=\"activation_2\"))\n",
    "            model.add(AveragePooling2D((1, 8), name=\"pooling_layer_2\"))\n",
    "            model.add(Dropout(0.5, name=\"drpout_2\")) #QUI DROPOUT E' LASCIATO A 0.5 come in eegnet paper\n",
    "\n",
    "            model.add(Flatten(name = 'flatten'))\n",
    "\n",
    "            model.add(Dense(numero_classi, name = 'dense', \n",
    "                                     kernel_constraint = max_norm(0.25)))\n",
    "            model.add(Activation('softmax', name = 'softmax'))\n",
    "\n",
    "\n",
    "            # Define the optimizer\n",
    "            optimizer= optimizers.Adam(\n",
    "            learning_rate= 1e-4,\n",
    "            weight_decay= 0\n",
    "            )\n",
    "            model.compile(optimizer=optimizer,\n",
    "                           loss=loss_fn,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "            evaluation.append(model.fit(X_train, y_train, batch_size=16,\n",
    "                      epochs=EPOCHS, \n",
    "                      validation_data=(X_test, y_test), \n",
    "                      verbose=0, workers=1)\n",
    "                         )\n",
    "\n",
    "            # Iteration = fold, i am just saving the model for that fold\n",
    "            iteration.append(model)\n",
    "\n",
    "            confusion_matrix_x_test.append(X_test)\n",
    "            confusion_matrix_y_test.append(y_test)\n",
    "\n",
    "            #Plotting confusion matrix\n",
    "            pred=model.predict(X_test)\n",
    "            y_test_pred= np.argmax(pred, axis=1)\n",
    "\n",
    "            confusion_matrix= metrics.confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "            plt.figure()\n",
    "            metrics.ConfusionMatrixDisplay(confusion_matrix).plot()\n",
    "            plt.savefig(dir1+\"/confusion_matrix_kfold_\"+str(count))\n",
    "            plt.close()\n",
    "\n",
    "            validation_acc.append(np.sum(y_test==y_test_pred)/y_test.shape[0])\n",
    "\n",
    "            PERFORMANCE.append(classification_report(y_test, y_test_pred, labels=LABELS, output_dict=True))\n",
    "\n",
    "            #Salvo risultati di singolo fold\n",
    "            sio.savemat(dir1+\"/y_pred_test_kfold\"+str(count), {\"array\": y_test_pred})\n",
    "            sio.savemat(dir1+\"/y_test_kfold\"+str(count), {\"array\": y_test})\n",
    "\n",
    "\n",
    "            # PLOTTO FILTRI TEMPORALI E SPAZIALI E LI SALVO\n",
    "            var= (model.get_layer(\"temporal_conv\").weights)\n",
    "            for lallo in range(8):\n",
    "                plt.figure()\n",
    "                plt.title(\"temp_conv_\"+str(lallo))\n",
    "                plt.plot(var[0][0,:,0][:,lallo]) #this way i access the temporal filters, cambiando ultimo zero\n",
    "                plt.savefig(dir2+\"/temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo))\n",
    "                temp= (var[0][0,:,0][:,lallo]).numpy()\n",
    "                sio.savemat(dir2+\"/temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo), {\"array\": temp})\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "            var_2= (model.get_layer(\"spatial_conv\").weights)\n",
    "            reshaped_var_2=tf.reshape(var_2[0][:,0,:,:],(31,16))\n",
    "            for lallo in range(16):\n",
    "                nump= reshaped_var_2[:,lallo].numpy()\n",
    "                sio.savemat(dir3+\"/spat_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo), {\"array\": nump})\n",
    "\n",
    "        ###################################################################################################################\n",
    "\n",
    "        #plot accuracy and loss function across epochs\n",
    "        epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "        min_temp_loss=10\n",
    "        min_temp_acc=10\n",
    "        max_temp_loss=0\n",
    "        max_temp_acc=0\n",
    "\n",
    "        for idx in range(n_folds):\n",
    "            if (np.min(evaluation[idx].history['loss'])<min_temp_loss):\n",
    "                min_temp_loss=np.min(evaluation[idx].history['loss'])\n",
    "            if (np.min(evaluation[idx].history['val_loss'])<min_temp_loss):\n",
    "                min_temp_loss=np.min(evaluation[idx].history['val_loss'])\n",
    "            if (np.max(evaluation[idx].history['loss'])>max_temp_loss):\n",
    "                max_temp_loss=np.max(evaluation[idx].history['loss'])\n",
    "            if (np.max(evaluation[idx].history['val_loss'])>max_temp_loss):\n",
    "                max_temp_loss=np.max(evaluation[idx].history['val_loss'])\n",
    "\n",
    "        for idx in range(n_folds):\n",
    "            if (np.min(evaluation[idx].history['accuracy'])<min_temp_acc):\n",
    "                min_temp_acc=np.min(evaluation[idx].history['accuracy'])\n",
    "            if (np.min(evaluation[idx].history['val_accuracy'])<min_temp_acc):\n",
    "                min_temp_acc=np.min(evaluation[idx].history['val_accuracy'])\n",
    "            if (np.max(evaluation[idx].history['accuracy'])>max_temp_acc):\n",
    "                max_temp_acc=np.max(evaluation[idx].history['accuracy'])\n",
    "            if (np.max(evaluation[idx].history['val_accuracy'])>max_temp_acc):\n",
    "                max_temp_acc=np.max(evaluation[idx].history['val_accuracy'])\n",
    "\n",
    "        for idx in range(n_folds):\n",
    "            loss_vec_train= evaluation[idx].history['loss']\n",
    "            loss_vec_test= evaluation[idx].history['val_loss']\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "            plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Loss across epochs for fold: '+str(idx))\n",
    "            plt.ylim([min_temp_loss, max_temp_loss])\n",
    "            plt.legend()\n",
    "\n",
    "            plt.savefig(dir1+\"/loss_kfold_\"+str(idx))\n",
    "            plt.close()\n",
    "\n",
    "        #plot accuracy and loss function across epochs\n",
    "        epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "\n",
    "        for idx in range(n_folds):\n",
    "            loss_vec_train= evaluation[idx].history['accuracy']\n",
    "            loss_vec_test= evaluation[idx].history['val_accuracy']\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "            plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Accuracy across epochs for fold: '+str(idx))\n",
    "            plt.ylim([min_temp_acc, max_temp_acc])\n",
    "            plt.legend()\n",
    "\n",
    "            plt.savefig(dir1+\"/accuracy_kfold_\"+str(idx))\n",
    "            plt.close()\n",
    "\n",
    "        #SALVO VARIABILI STATISTICHE E MODELLO IN MODO DA PLOTTARLO\n",
    "\n",
    "#         acc_temp=[]\n",
    "        accuratezza=np.zeros(n_classes*2)\n",
    "#         for idx in range(n_folds):\n",
    "#             acc_temp.append(PERFORMANCE[idx][\"accuracy\"])\n",
    "        accuratezza[0]=(np.mean(validation_acc))\n",
    "        accuratezza[1]=(statistics.pstdev(validation_acc))\n",
    "\n",
    "\n",
    "        precisione=[]\n",
    "        recall=[]\n",
    "        f1_score=[]\n",
    "        support=[]\n",
    "        for classe in range(n_classes):\n",
    "            precision_temp=[]\n",
    "            recall_temp=[]\n",
    "            f1_score_temp=[]\n",
    "            support_temp=[]\n",
    "            for idx in range(n_folds):\n",
    "                precision_temp.append(PERFORMANCE[idx][str(classe)][\"precision\"])\n",
    "                recall_temp.append(PERFORMANCE[idx][str(classe)][\"recall\"])\n",
    "                f1_score_temp.append(PERFORMANCE[idx][str(classe)][\"f1-score\"])\n",
    "                support_temp.append(PERFORMANCE[idx][str(classe)][\"support\"])\n",
    "\n",
    "            precisione.append(np.mean(precision_temp))\n",
    "            precisione.append(statistics.pstdev(precision_temp))\n",
    "            recall.append(np.mean(recall_temp))\n",
    "            recall.append(statistics.pstdev(recall_temp))\n",
    "            f1_score.append(np.mean(f1_score_temp))\n",
    "            f1_score.append(statistics.pstdev(f1_score_temp))    \n",
    "            support.append(np.mean(support_temp))\n",
    "            support.append(statistics.pstdev(support_temp)) \n",
    "\n",
    "        sommario=[]\n",
    "        sommario=np.vstack((accuratezza,precisione,recall,f1_score,recall))\n",
    "        sio.savemat(dir0+\"/sommario.mat\", {\"array\": sommario})\n",
    "\n",
    "        gianfranco=model.predict(X)\n",
    "        gianfranco2=np.argmax(gianfranco, axis=1)\n",
    "        sio.savemat(dir0+\"/predizione_totale.mat\", {\"array\": gianfranco2})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bfc4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER SUB & PROTOCOL\n",
    "SUBJECTS=[6,7,8,9,11,12,13,14,16,17]\n",
    "for temp_sub in SUBJECTS:\n",
    "    CLASSES= [2,3,5,10,50]\n",
    "    #questa va applicata a for loop di subject che deve essere il pi첫 esterno \n",
    "    sub=\"{:02d}\".format(temp_sub)\n",
    "\n",
    "    dir00= os.path.join(\"FINAL_fuzzy_regressor/MinMax_per_sub_protocol/output\", \"sub_\"+str(sub))\n",
    "    os.mkdir(dir00)\n",
    "    for n_classes in CLASSES:\n",
    "        #Define loss function\n",
    "        loss_fn= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "        LABELS= list(range(0,n_classes))\n",
    "        \n",
    "        if n_classes==50:\n",
    "            numero_classi=5\n",
    "        else:\n",
    "            numero_classi=n_classes\n",
    "\n",
    "        evaluation=[]\n",
    "        iteration=[]\n",
    "        confusion_matrix_x_test=[]\n",
    "        confusion_matrix_y_test= []\n",
    "        validation_acc=[]\n",
    "        PERFORMANCE=[]\n",
    "\n",
    "        print(\"SUBJECT: \"+ str(sub))\n",
    "        print(\"N_CLASSES: \"+ str(n_classes))\n",
    "        \n",
    "        X=np.load(\"input/sub_\"+str(sub)+ \"_compiled_RAW_downsampled_float32.npy\")\n",
    "        y= sio.loadmat(\"FINAL_fuzzy_regressor/MinMax_per_sub_protocol/input/sub-\"+str(sub)+\"_compiled_rightSMA_scaled_per_sub_protocol_\"+str(n_classes)+\"class_balanced.mat\")[\"compiled_rightSMA_labels\"]\n",
    "\n",
    "        X= X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "        y=y.flatten()\n",
    "        print(\"New shape for X: \" + str(X.shape))\n",
    "        print(\"New shape for y: \"+str(y.shape))\n",
    "\n",
    "        dir0= os.path.join(dir00, str(n_classes)+\"_class\")\n",
    "        os.mkdir(dir0)\n",
    "        dir1=os.path.join(dir0, \"comparison\")\n",
    "        dir2=os.path.join(dir0, \"temporal_convolution\")\n",
    "        dir3=os.path.join(dir0, \"spatial_convolution\")\n",
    "        os.mkdir(dir1)\n",
    "        os.mkdir(dir2)\n",
    "        os.mkdir(dir3)\n",
    "\n",
    "        ################################################################################################################################################################################################################################################################################\n",
    "\n",
    "        n_folds = 5\n",
    "        seed = 24\n",
    "        shuffle_test = True\n",
    "        EPOCHS=250\n",
    "\n",
    "        kfold = KFold(n_splits = n_folds, shuffle = shuffle_test, random_state = seed)\n",
    "        count=0\n",
    "        sommatoria=0\n",
    "\n",
    "        for train_index, test_index in kfold.split(X):\n",
    "            count=count+1\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            print(\"Train index for this split: \"+ str(train_index)) \n",
    "            print(\"Number of samples for train set: \"+str(train_index.shape[0]))\n",
    "            print(\"Test index for this split: \"+ str(test_index))\n",
    "            print(\"Number of samples for test set: \"+str(test_index.shape[0]))\n",
    "\n",
    "            # Define the model architecture - \n",
    "\n",
    "            model=Sequential()\n",
    "\n",
    "            ##################################################################\n",
    "\n",
    "            model.add(Conv2D(8, (1, 64), padding = 'same',\n",
    "                                           input_shape = (N_chan, N_samples_long, 1),\n",
    "                                           use_bias = False, name=\"temporal_conv\"))\n",
    "            model.add(BatchNormalization(name=\"batchnorm_1\"))\n",
    "            model.add(DepthwiseConv2D((31, 1), use_bias = False, \n",
    "                                           depth_multiplier = 2,\n",
    "                                           depthwise_constraint = max_norm(1.), name=\"spatial_conv\"))\n",
    "            model.add(BatchNormalization(name=\"batchnorm_2\"))\n",
    "            model.add(Activation('elu', name=\"activation_1\"))\n",
    "            model.add(AveragePooling2D((1, 4), name=\"pooling_layer_1\"))\n",
    "            model.add(Dropout(0.5, name=\"dropout_1\"))\n",
    "\n",
    "            model.add(SeparableConv2D(16, (1, 16),\n",
    "                                           use_bias = False, padding = 'same', name=\"separable_conv\"))\n",
    "            model.add(BatchNormalization(name=\"batchnorm_3\"))\n",
    "            model.add(Activation('elu', name=\"activation_2\"))\n",
    "            model.add(AveragePooling2D((1, 8), name=\"pooling_layer_2\"))\n",
    "            model.add(Dropout(0.5, name=\"drpout_2\")) #QUI DROPOUT E' LASCIATO A 0.5 come in eegnet paper\n",
    "\n",
    "            model.add(Flatten(name = 'flatten'))\n",
    "\n",
    "            model.add(Dense(numero_classi, name = 'dense', \n",
    "                                     kernel_constraint = max_norm(0.25)))\n",
    "            model.add(Activation('softmax', name = 'softmax'))\n",
    "\n",
    "\n",
    "            # Define the optimizer\n",
    "            optimizer= optimizers.Adam(\n",
    "            learning_rate= 1e-4,\n",
    "            weight_decay= 0\n",
    "            )\n",
    "            model.compile(optimizer=optimizer,\n",
    "                           loss=loss_fn,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "            evaluation.append(model.fit(X_train, y_train, batch_size=16,\n",
    "                      epochs=EPOCHS, \n",
    "                      validation_data=(X_test, y_test), \n",
    "                      verbose=0, workers=1)\n",
    "                         )\n",
    "\n",
    "            # Iteration = fold, i am just saving the model for that fold\n",
    "            iteration.append(model)\n",
    "\n",
    "            confusion_matrix_x_test.append(X_test)\n",
    "            confusion_matrix_y_test.append(y_test)\n",
    "\n",
    "            #Plotting confusion matrix\n",
    "            pred=model.predict(X_test)\n",
    "            y_test_pred= np.argmax(pred, axis=1)\n",
    "\n",
    "            confusion_matrix= metrics.confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "            plt.figure()\n",
    "            metrics.ConfusionMatrixDisplay(confusion_matrix).plot()\n",
    "            plt.savefig(dir1+\"/confusion_matrix_kfold_\"+str(count))\n",
    "            plt.close()\n",
    "\n",
    "            validation_acc.append(np.sum(y_test==y_test_pred)/y_test.shape[0])\n",
    "\n",
    "            PERFORMANCE.append(classification_report(y_test, y_test_pred, labels=LABELS, output_dict=True))\n",
    "\n",
    "            #Salvo risultati di singolo fold\n",
    "            sio.savemat(dir1+\"/y_pred_test_kfold\"+str(count), {\"array\": y_test_pred})\n",
    "            sio.savemat(dir1+\"/y_test_kfold\"+str(count), {\"array\": y_test})\n",
    "\n",
    "\n",
    "            # PLOTTO FILTRI TEMPORALI E SPAZIALI E LI SALVO\n",
    "            var= (model.get_layer(\"temporal_conv\").weights)\n",
    "            for lallo in range(8):\n",
    "                plt.figure()\n",
    "                plt.title(\"temp_conv_\"+str(lallo))\n",
    "                plt.plot(var[0][0,:,0][:,lallo]) #this way i access the temporal filters, cambiando ultimo zero\n",
    "                plt.savefig(dir2+\"/temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo))\n",
    "                temp= (var[0][0,:,0][:,lallo]).numpy()\n",
    "                sio.savemat(dir2+\"/temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo), {\"array\": temp})\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "            var_2= (model.get_layer(\"spatial_conv\").weights)\n",
    "            reshaped_var_2=tf.reshape(var_2[0][:,0,:,:],(31,16))\n",
    "            for lallo in range(16):\n",
    "                nump= reshaped_var_2[:,lallo].numpy()\n",
    "                sio.savemat(dir3+\"/spat_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo), {\"array\": nump})\n",
    "\n",
    "        ###################################################################################################################\n",
    "\n",
    "        #plot accuracy and loss function across epochs\n",
    "        epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "        min_temp_loss=10\n",
    "        min_temp_acc=10\n",
    "        max_temp_loss=0\n",
    "        max_temp_acc=0\n",
    "\n",
    "        for idx in range(n_folds):\n",
    "            if (np.min(evaluation[idx].history['loss'])<min_temp_loss):\n",
    "                min_temp_loss=np.min(evaluation[idx].history['loss'])\n",
    "            if (np.min(evaluation[idx].history['val_loss'])<min_temp_loss):\n",
    "                min_temp_loss=np.min(evaluation[idx].history['val_loss'])\n",
    "            if (np.max(evaluation[idx].history['loss'])>max_temp_loss):\n",
    "                max_temp_loss=np.max(evaluation[idx].history['loss'])\n",
    "            if (np.max(evaluation[idx].history['val_loss'])>max_temp_loss):\n",
    "                max_temp_loss=np.max(evaluation[idx].history['val_loss'])\n",
    "\n",
    "        for idx in range(n_folds):\n",
    "            if (np.min(evaluation[idx].history['accuracy'])<min_temp_acc):\n",
    "                min_temp_acc=np.min(evaluation[idx].history['accuracy'])\n",
    "            if (np.min(evaluation[idx].history['val_accuracy'])<min_temp_acc):\n",
    "                min_temp_acc=np.min(evaluation[idx].history['val_accuracy'])\n",
    "            if (np.max(evaluation[idx].history['accuracy'])>max_temp_acc):\n",
    "                max_temp_acc=np.max(evaluation[idx].history['accuracy'])\n",
    "            if (np.max(evaluation[idx].history['val_accuracy'])>max_temp_acc):\n",
    "                max_temp_acc=np.max(evaluation[idx].history['val_accuracy'])\n",
    "\n",
    "        for idx in range(n_folds):\n",
    "            loss_vec_train= evaluation[idx].history['loss']\n",
    "            loss_vec_test= evaluation[idx].history['val_loss']\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "            plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Loss across epochs for fold: '+str(idx))\n",
    "            plt.ylim([min_temp_loss, max_temp_loss])\n",
    "            plt.legend()\n",
    "\n",
    "            plt.savefig(dir1+\"/loss_kfold_\"+str(idx))\n",
    "            plt.close()\n",
    "\n",
    "        #plot accuracy and loss function across epochs\n",
    "        epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "\n",
    "        for idx in range(n_folds):\n",
    "            loss_vec_train= evaluation[idx].history['accuracy']\n",
    "            loss_vec_test= evaluation[idx].history['val_accuracy']\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "            plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Accuracy across epochs for fold: '+str(idx))\n",
    "            plt.ylim([min_temp_acc, max_temp_acc])\n",
    "            plt.legend()\n",
    "\n",
    "            plt.savefig(dir1+\"/accuracy_kfold_\"+str(idx))\n",
    "            plt.close()\n",
    "\n",
    "        #SALVO VARIABILI STATISTICHE E MODELLO IN MODO DA PLOTTARLO\n",
    "\n",
    "#         acc_temp=[]\n",
    "        accuratezza=np.zeros(n_classes*2)\n",
    "#         for idx in range(n_folds):\n",
    "#             acc_temp.append(PERFORMANCE[idx][\"accuracy\"])\n",
    "        accuratezza[0]=(np.mean(validation_acc))\n",
    "        accuratezza[1]=(statistics.pstdev(validation_acc))\n",
    "\n",
    "\n",
    "        precisione=[]\n",
    "        recall=[]\n",
    "        f1_score=[]\n",
    "        support=[]\n",
    "        for classe in range(n_classes):\n",
    "            precision_temp=[]\n",
    "            recall_temp=[]\n",
    "            f1_score_temp=[]\n",
    "            support_temp=[]\n",
    "            for idx in range(n_folds):\n",
    "                precision_temp.append(PERFORMANCE[idx][str(classe)][\"precision\"])\n",
    "                recall_temp.append(PERFORMANCE[idx][str(classe)][\"recall\"])\n",
    "                f1_score_temp.append(PERFORMANCE[idx][str(classe)][\"f1-score\"])\n",
    "                support_temp.append(PERFORMANCE[idx][str(classe)][\"support\"])\n",
    "\n",
    "            precisione.append(np.mean(precision_temp))\n",
    "            precisione.append(statistics.pstdev(precision_temp))\n",
    "            recall.append(np.mean(recall_temp))\n",
    "            recall.append(statistics.pstdev(recall_temp))\n",
    "            f1_score.append(np.mean(f1_score_temp))\n",
    "            f1_score.append(statistics.pstdev(f1_score_temp))    \n",
    "            support.append(np.mean(support_temp))\n",
    "            support.append(statistics.pstdev(support_temp)) \n",
    "\n",
    "        sommario=[]\n",
    "        sommario=np.vstack((accuratezza,precisione,recall,f1_score,recall))\n",
    "        sio.savemat(dir0+\"/sommario.mat\", {\"array\": sommario})\n",
    "\n",
    "        gianfranco=model.predict(X)\n",
    "        gianfranco2=np.argmax(gianfranco, axis=1)\n",
    "        sio.savemat(dir0+\"/predizione_totale.mat\", {\"array\": gianfranco2})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ece303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER RUN \n",
    "# SUBJECTS=[3,4,5,6,7,8,9,11,12,13,14,16,17]\n",
    "SUBJECTS=[17]\n",
    "for temp_sub in SUBJECTS:\n",
    "#     CLASSES= [2,3,5,10,50]\n",
    "    CLASSES= [3,5,10,50]\n",
    "    #questa va applicata a for loop di subject che deve essere il pi첫 esterno \n",
    "    sub=\"{:02d}\".format(temp_sub)\n",
    "\n",
    "    dir00= os.path.join(\"FINAL_fuzzy_regressor/MinMax_per_run/output\", \"sub_\"+str(sub))\n",
    "#     os.mkdir(dir00)\n",
    "    for n_classes in CLASSES:\n",
    "        #Define loss function\n",
    "        loss_fn= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "        LABELS= list(range(0,n_classes))\n",
    "        \n",
    "        if n_classes==50:\n",
    "            numero_classi=5\n",
    "        else:\n",
    "            numero_classi=n_classes\n",
    "\n",
    "        evaluation=[]\n",
    "        iteration=[]\n",
    "        confusion_matrix_x_test=[]\n",
    "        confusion_matrix_y_test= []\n",
    "        validation_acc=[]\n",
    "        PERFORMANCE=[]\n",
    "\n",
    "        print(\"SUBJECT: \"+ str(sub))\n",
    "        print(\"N_CLASSES: \"+ str(n_classes))\n",
    "        \n",
    "        X=np.load(\"input/sub_\"+str(sub)+ \"_compiled_RAW_downsampled_float32.npy\")\n",
    "        y= sio.loadmat(\"FINAL_fuzzy_regressor/MinMax_per_run/input/sub-\"+str(sub)+\"_compiled_rightSMA_scaled_per_run_\"+str(n_classes)+\"class_balanced.mat\")[\"compiled_rightSMA_labels\"]\n",
    "\n",
    "        X= X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "        y=y.flatten()\n",
    "        print(\"New shape for X: \" + str(X.shape))\n",
    "        print(\"New shape for y: \"+str(y.shape))\n",
    "\n",
    "        dir0= os.path.join(dir00, str(n_classes)+\"_class\")\n",
    "        os.mkdir(dir0)\n",
    "        dir1=os.path.join(dir0, \"comparison\")\n",
    "        dir2=os.path.join(dir0, \"temporal_convolution\")\n",
    "        dir3=os.path.join(dir0, \"spatial_convolution\")\n",
    "        os.mkdir(dir1)\n",
    "        os.mkdir(dir2)\n",
    "        os.mkdir(dir3)\n",
    "\n",
    "        ################################################################################################################################################################################################################################################################################\n",
    "\n",
    "        n_folds = 5\n",
    "#         seed = 21\n",
    "        shuffle_test = True\n",
    "        EPOCHS=250\n",
    "\n",
    "        kfold = KFold(n_splits = n_folds, shuffle = shuffle_test)\n",
    "        count=0\n",
    "        sommatoria=0\n",
    "\n",
    "        for train_index, test_index in kfold.split(X):\n",
    "            count=count+1\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            print(\"Train index for this split: \"+ str(train_index)) \n",
    "            print(\"Number of samples for train set: \"+str(train_index.shape[0]))\n",
    "            print(\"Test index for this split: \"+ str(test_index))\n",
    "            print(\"Number of samples for test set: \"+str(test_index.shape[0]))\n",
    "\n",
    "            # Define the model architecture - \n",
    "\n",
    "            model=Sequential()\n",
    "\n",
    "            ##################################################################\n",
    "\n",
    "            model.add(Conv2D(8, (1, 64), padding = 'same',\n",
    "                                           input_shape = (N_chan, N_samples_long, 1),\n",
    "                                           use_bias = False, name=\"temporal_conv\"))\n",
    "            model.add(BatchNormalization(name=\"batchnorm_1\"))\n",
    "            model.add(DepthwiseConv2D((31, 1), use_bias = False, \n",
    "                                           depth_multiplier = 2,\n",
    "                                           depthwise_constraint = max_norm(1.), name=\"spatial_conv\"))\n",
    "            model.add(BatchNormalization(name=\"batchnorm_2\"))\n",
    "            model.add(Activation('elu', name=\"activation_1\"))\n",
    "            model.add(AveragePooling2D((1, 4), name=\"pooling_layer_1\"))\n",
    "            model.add(Dropout(0.5, name=\"dropout_1\"))\n",
    "\n",
    "            model.add(SeparableConv2D(16, (1, 16),\n",
    "                                           use_bias = False, padding = 'same', name=\"separable_conv\"))\n",
    "            model.add(BatchNormalization(name=\"batchnorm_3\"))\n",
    "            model.add(Activation('elu', name=\"activation_2\"))\n",
    "            model.add(AveragePooling2D((1, 8), name=\"pooling_layer_2\"))\n",
    "            model.add(Dropout(0.5, name=\"drpout_2\")) #QUI DROPOUT E' LASCIATO A 0.5 come in eegnet paper\n",
    "\n",
    "            model.add(Flatten(name = 'flatten'))\n",
    "\n",
    "            model.add(Dense(numero_classi, name = 'dense', \n",
    "                                     kernel_constraint = max_norm(0.25)))\n",
    "            model.add(Activation('softmax', name = 'softmax'))\n",
    "\n",
    "\n",
    "            # Define the optimizer\n",
    "            optimizer= optimizers.Adam(\n",
    "            learning_rate= 1e-4,\n",
    "            weight_decay= 0\n",
    "            )\n",
    "            model.compile(optimizer=optimizer,\n",
    "                           loss=loss_fn,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "            evaluation.append(model.fit(X_train, y_train, batch_size=16,\n",
    "                      epochs=EPOCHS, \n",
    "                      validation_data=(X_test, y_test), \n",
    "                      verbose=0, workers=1)\n",
    "                         )\n",
    "\n",
    "            # Iteration = fold, i am just saving the model for that fold\n",
    "            iteration.append(model)\n",
    "\n",
    "            confusion_matrix_x_test.append(X_test)\n",
    "            confusion_matrix_y_test.append(y_test)\n",
    "\n",
    "            #Plotting confusion matrix\n",
    "            pred=model.predict(X_test)\n",
    "            y_test_pred= np.argmax(pred, axis=1)\n",
    "\n",
    "            confusion_matrix= metrics.confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "            plt.figure()\n",
    "            metrics.ConfusionMatrixDisplay(confusion_matrix).plot()\n",
    "            plt.savefig(dir1+\"/confusion_matrix_kfold_\"+str(count))\n",
    "            plt.close()\n",
    "\n",
    "            validation_acc.append(np.sum(y_test==y_test_pred)/y_test.shape[0])\n",
    "\n",
    "            PERFORMANCE.append(classification_report(y_test, y_test_pred, labels=LABELS, output_dict=True))\n",
    "\n",
    "            #Salvo risultati di singolo fold\n",
    "            sio.savemat(dir1+\"/y_pred_test_kfold\"+str(count), {\"array\": y_test_pred})\n",
    "            sio.savemat(dir1+\"/y_test_kfold\"+str(count), {\"array\": y_test})\n",
    "\n",
    "\n",
    "            # PLOTTO FILTRI TEMPORALI E SPAZIALI E LI SALVO\n",
    "            var= (model.get_layer(\"temporal_conv\").weights)\n",
    "            for lallo in range(8):\n",
    "                plt.figure()\n",
    "                plt.title(\"temp_conv_\"+str(lallo))\n",
    "                plt.plot(var[0][0,:,0][:,lallo]) #this way i access the temporal filters, cambiando ultimo zero\n",
    "                plt.savefig(dir2+\"/temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo))\n",
    "                temp= (var[0][0,:,0][:,lallo]).numpy()\n",
    "                sio.savemat(dir2+\"/temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo), {\"array\": temp})\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "            var_2= (model.get_layer(\"spatial_conv\").weights)\n",
    "            reshaped_var_2=tf.reshape(var_2[0][:,0,:,:],(31,16))\n",
    "            for lallo in range(16):\n",
    "                nump= reshaped_var_2[:,lallo].numpy()\n",
    "                sio.savemat(dir3+\"/spat_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo), {\"array\": nump})\n",
    "\n",
    "        ###################################################################################################################\n",
    "\n",
    "        #plot accuracy and loss function across epochs\n",
    "        epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "        min_temp_loss=10\n",
    "        min_temp_acc=10\n",
    "        max_temp_loss=0\n",
    "        max_temp_acc=0\n",
    "\n",
    "        for idx in range(n_folds):\n",
    "            if (np.min(evaluation[idx].history['loss'])<min_temp_loss):\n",
    "                min_temp_loss=np.min(evaluation[idx].history['loss'])\n",
    "            if (np.min(evaluation[idx].history['val_loss'])<min_temp_loss):\n",
    "                min_temp_loss=np.min(evaluation[idx].history['val_loss'])\n",
    "            if (np.max(evaluation[idx].history['loss'])>max_temp_loss):\n",
    "                max_temp_loss=np.max(evaluation[idx].history['loss'])\n",
    "            if (np.max(evaluation[idx].history['val_loss'])>max_temp_loss):\n",
    "                max_temp_loss=np.max(evaluation[idx].history['val_loss'])\n",
    "\n",
    "        for idx in range(n_folds):\n",
    "            if (np.min(evaluation[idx].history['accuracy'])<min_temp_acc):\n",
    "                min_temp_acc=np.min(evaluation[idx].history['accuracy'])\n",
    "            if (np.min(evaluation[idx].history['val_accuracy'])<min_temp_acc):\n",
    "                min_temp_acc=np.min(evaluation[idx].history['val_accuracy'])\n",
    "            if (np.max(evaluation[idx].history['accuracy'])>max_temp_acc):\n",
    "                max_temp_acc=np.max(evaluation[idx].history['accuracy'])\n",
    "            if (np.max(evaluation[idx].history['val_accuracy'])>max_temp_acc):\n",
    "                max_temp_acc=np.max(evaluation[idx].history['val_accuracy'])\n",
    "\n",
    "        for idx in range(n_folds):\n",
    "            loss_vec_train= evaluation[idx].history['loss']\n",
    "            loss_vec_test= evaluation[idx].history['val_loss']\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "            plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Loss across epochs for fold: '+str(idx))\n",
    "            plt.ylim([min_temp_loss, max_temp_loss])\n",
    "            plt.legend()\n",
    "\n",
    "            plt.savefig(dir1+\"/loss_kfold_\"+str(idx))\n",
    "            plt.close()\n",
    "\n",
    "        #plot accuracy and loss function across epochs\n",
    "        epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "\n",
    "        for idx in range(n_folds):\n",
    "            loss_vec_train= evaluation[idx].history['accuracy']\n",
    "            loss_vec_test= evaluation[idx].history['val_accuracy']\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "            plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Accuracy across epochs for fold: '+str(idx))\n",
    "            plt.ylim([min_temp_acc, max_temp_acc])\n",
    "            plt.legend()\n",
    "\n",
    "            plt.savefig(dir1+\"/accuracy_kfold_\"+str(idx))\n",
    "            plt.close()\n",
    "\n",
    "        #SALVO VARIABILI STATISTICHE E MODELLO IN MODO DA PLOTTARLO\n",
    "\n",
    "#         acc_temp=[]\n",
    "        accuratezza=np.zeros(n_classes*2)\n",
    "#         for idx in range(n_folds):\n",
    "#             acc_temp.append(PERFORMANCE[idx][\"accuracy\"])\n",
    "        accuratezza[0]=(np.mean(validation_acc))\n",
    "        accuratezza[1]=(statistics.pstdev(validation_acc))\n",
    "\n",
    "\n",
    "        precisione=[]\n",
    "        recall=[]\n",
    "        f1_score=[]\n",
    "        support=[]\n",
    "        for classe in range(numero_classi):\n",
    "            precision_temp=[]\n",
    "            recall_temp=[]\n",
    "            f1_score_temp=[]\n",
    "            support_temp=[]\n",
    "            for idx in range(n_folds):\n",
    "                precision_temp.append(PERFORMANCE[idx][str(classe)][\"precision\"])\n",
    "                recall_temp.append(PERFORMANCE[idx][str(classe)][\"recall\"])\n",
    "                f1_score_temp.append(PERFORMANCE[idx][str(classe)][\"f1-score\"])\n",
    "                support_temp.append(PERFORMANCE[idx][str(classe)][\"support\"])\n",
    "\n",
    "            precisione.append(np.mean(precision_temp))\n",
    "            precisione.append(statistics.pstdev(precision_temp))\n",
    "            recall.append(np.mean(recall_temp))\n",
    "            recall.append(statistics.pstdev(recall_temp))\n",
    "            f1_score.append(np.mean(f1_score_temp))\n",
    "            f1_score.append(statistics.pstdev(f1_score_temp))    \n",
    "            support.append(np.mean(support_temp))\n",
    "            support.append(statistics.pstdev(support_temp)) \n",
    "\n",
    "        sommario=[]\n",
    "        sommario=np.vstack((accuratezza,precisione,recall,f1_score,recall))\n",
    "        sio.savemat(dir0+\"/sommario.mat\", {\"array\": sommario})\n",
    "\n",
    "        gianfranco=model.predict(X)\n",
    "        gianfranco2=np.argmax(gianfranco, axis=1)\n",
    "        sio.savemat(dir0+\"/predizione_totale.mat\", {\"array\": gianfranco2})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e6cdc9",
   "metadata": {},
   "source": [
    "# ALL-per SUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER SUB \n",
    "# ########SUBJECTS=[1,2,3,4,5,6,7,8,9,11,12,13,14,16,17]\n",
    "# SUBJECTS=[11,12,13,14,16,17]\n",
    "# for temp_sub in SUBJECTS:\n",
    "CLASSES= [2,3,5,10,50]\n",
    "#questa va applicata a for loop di subject che deve essere il pi첫 esterno \n",
    "#     sub=\"{:02d}\".format(temp_sub)\n",
    "\n",
    "dir00= os.path.join(\"FINAL_fuzzy_regressor/MinMax_per_sub/output\", \"all\")\n",
    "os.mkdir(dir00)\n",
    "for n_classes in CLASSES:\n",
    "    #Define loss function\n",
    "    loss_fn= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    LABELS= list(range(0,n_classes))\n",
    "\n",
    "    if n_classes==50:\n",
    "        numero_classi=5\n",
    "    else:\n",
    "        numero_classi=n_classes\n",
    "\n",
    "    evaluation=[]\n",
    "    iteration=[]\n",
    "    confusion_matrix_x_test=[]\n",
    "    confusion_matrix_y_test= []\n",
    "    validation_acc=[]\n",
    "    PERFORMANCE=[]\n",
    "\n",
    "    X=np.load(\"input/RAW_regression_all_subs_float32.npy\")\n",
    "    y= sio.loadmat(\"FINAL_fuzzy_regressor/MinMax_per_sub/input/Compiled_rightSMA_all_scaled_per_sub_\"+str(n_classes)+\"class_balanced.mat\")[\"compiled_rightSMA_labels_all\"]\n",
    "\n",
    "    X= X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "    y=y.flatten()\n",
    "    print(\"Shape for X: \" + str(X.shape))\n",
    "    print(\"Shape for y: \"+str(y.shape))\n",
    "\n",
    "    X= X[762:,:,:,:]\n",
    "    y= y[762:]\n",
    "    print(\"New shape for X: \" + str(X.shape))\n",
    "    print(\"New shape for y: \"+str(y.shape))\n",
    "    \n",
    "    dir0= os.path.join(dir00, str(n_classes)+\"_class\")\n",
    "    os.mkdir(dir0)\n",
    "    dir1=os.path.join(dir0, \"comparison\")\n",
    "    dir2=os.path.join(dir0, \"temporal_convolution\")\n",
    "    dir3=os.path.join(dir0, \"spatial_convolution\")\n",
    "    os.mkdir(dir1)\n",
    "    os.mkdir(dir2)\n",
    "    os.mkdir(dir3)\n",
    "\n",
    "    ################################################################################################################################################################################################################################################################################\n",
    "\n",
    "    n_folds = 5\n",
    "    seed = 21\n",
    "    shuffle_test = True\n",
    "    EPOCHS=250\n",
    "    DROPOUT_TEST=0.25\n",
    "\n",
    "    kfold = KFold(n_splits = n_folds, shuffle = shuffle_test, random_state = seed)\n",
    "    count=0\n",
    "    sommatoria=0\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        count=count+1\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        print(\"Train index for this split: \"+ str(train_index)) \n",
    "        print(\"Number of samples for train set: \"+str(train_index.shape[0]))\n",
    "        print(\"Test index for this split: \"+ str(test_index))\n",
    "        print(\"Number of samples for test set: \"+str(test_index.shape[0]))\n",
    "\n",
    "        # Define the model architecture - \n",
    "\n",
    "        model=Sequential()\n",
    "\n",
    "        ##################################################################\n",
    "\n",
    "        model.add(Conv2D(8, (1, 64), padding = 'same',\n",
    "                                       input_shape = (N_chan, N_samples_long, 1),\n",
    "                                       use_bias = False, name=\"temporal_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_1\"))\n",
    "        model.add(DepthwiseConv2D((31, 1), use_bias = False, \n",
    "                                       depth_multiplier = 2,\n",
    "                                       depthwise_constraint = max_norm(1.), name=\"spatial_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_2\"))\n",
    "        model.add(Activation('elu', name=\"activation_1\"))\n",
    "        model.add(AveragePooling2D((1, 4), name=\"pooling_layer_1\"))\n",
    "        model.add(Dropout(0.5, name=\"dropout_1\"))\n",
    "\n",
    "        model.add(SeparableConv2D(16, (1, 16),\n",
    "                                       use_bias = False, padding = 'same', name=\"separable_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_3\"))\n",
    "        model.add(Activation('elu', name=\"activation_2\"))\n",
    "        model.add(AveragePooling2D((1, 8), name=\"pooling_layer_2\"))\n",
    "        model.add(Dropout(DROPOUT_TEST, name=\"drpout_2\")) #QUI DROPOUT E' LASCIATO A 0.5 come in eegnet paper\n",
    "\n",
    "        model.add(Flatten(name = 'flatten'))\n",
    "\n",
    "        model.add(Dense(numero_classi, name = 'dense', \n",
    "                                 kernel_constraint = max_norm(0.25)))\n",
    "        model.add(Activation('softmax', name = 'softmax'))\n",
    "\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer= optimizers.Adam(\n",
    "        learning_rate= 1e-4,\n",
    "        weight_decay= 0\n",
    "        )\n",
    "        model.compile(optimizer=optimizer,\n",
    "                       loss=loss_fn,\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "        evaluation.append(model.fit(X_train, y_train,\n",
    "                  epochs=EPOCHS, \n",
    "                  validation_data=(X_test, y_test), \n",
    "                  verbose=2, workers=1)\n",
    "                     )\n",
    "\n",
    "        # Iteration = fold, i am just saving the model for that fold\n",
    "        iteration.append(model)\n",
    "\n",
    "        confusion_matrix_x_test.append(X_test)\n",
    "        confusion_matrix_y_test.append(y_test)\n",
    "\n",
    "        #Plotting confusion matrix\n",
    "        pred=model.predict(X_test)\n",
    "        y_test_pred= np.argmax(pred, axis=1)\n",
    "\n",
    "        confusion_matrix= metrics.confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "        plt.figure()\n",
    "        metrics.ConfusionMatrixDisplay(confusion_matrix).plot()\n",
    "        plt.savefig(dir1+\"/confusion_matrix_kfold_\"+str(count))\n",
    "        plt.close()\n",
    "\n",
    "        validation_acc.append(np.sum(y_test==y_test_pred)/y_test.shape[0])\n",
    "\n",
    "        PERFORMANCE.append(classification_report(y_test, y_test_pred, labels=LABELS, output_dict=True))\n",
    "\n",
    "        #Salvo risultati di singolo fold\n",
    "        sio.savemat(dir1+\"/y_pred_test_kfold\"+str(count), {\"array\": y_test_pred})\n",
    "        sio.savemat(dir1+\"/y_test_kfold\"+str(count), {\"array\": y_test})\n",
    "\n",
    "\n",
    "        # PLOTTO FILTRI TEMPORALI E SPAZIALI E LI SALVO\n",
    "        var= (model.get_layer(\"temporal_conv\").weights)\n",
    "        for lallo in range(8):\n",
    "            plt.figure()\n",
    "            plt.title(\"temp_conv_\"+str(lallo))\n",
    "            plt.plot(var[0][0,:,0][:,lallo]) #this way i access the temporal filters, cambiando ultimo zero\n",
    "            plt.savefig(dir2+\"/temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo))\n",
    "            temp= (var[0][0,:,0][:,lallo]).numpy()\n",
    "            sio.savemat(dir2+\"/temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo), {\"array\": temp})\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        var_2= (model.get_layer(\"spatial_conv\").weights)\n",
    "        reshaped_var_2=tf.reshape(var_2[0][:,0,:,:],(31,16))\n",
    "        for lallo in range(16):\n",
    "            nump= reshaped_var_2[:,lallo].numpy()\n",
    "            sio.savemat(dir3+\"/spat_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo), {\"array\": nump})\n",
    "\n",
    "    ###################################################################################################################\n",
    "\n",
    "    #plot accuracy and loss function across epochs\n",
    "    epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "    min_temp_loss=10\n",
    "    min_temp_acc=10\n",
    "    max_temp_loss=0\n",
    "    max_temp_acc=0\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        if (np.min(evaluation[idx].history['loss'])<min_temp_loss):\n",
    "            min_temp_loss=np.min(evaluation[idx].history['loss'])\n",
    "        if (np.min(evaluation[idx].history['val_loss'])<min_temp_loss):\n",
    "            min_temp_loss=np.min(evaluation[idx].history['val_loss'])\n",
    "        if (np.max(evaluation[idx].history['loss'])>max_temp_loss):\n",
    "            max_temp_loss=np.max(evaluation[idx].history['loss'])\n",
    "        if (np.max(evaluation[idx].history['val_loss'])>max_temp_loss):\n",
    "            max_temp_loss=np.max(evaluation[idx].history['val_loss'])\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        if (np.min(evaluation[idx].history['accuracy'])<min_temp_acc):\n",
    "            min_temp_acc=np.min(evaluation[idx].history['accuracy'])\n",
    "        if (np.min(evaluation[idx].history['val_accuracy'])<min_temp_acc):\n",
    "            min_temp_acc=np.min(evaluation[idx].history['val_accuracy'])\n",
    "        if (np.max(evaluation[idx].history['accuracy'])>max_temp_acc):\n",
    "            max_temp_acc=np.max(evaluation[idx].history['accuracy'])\n",
    "        if (np.max(evaluation[idx].history['val_accuracy'])>max_temp_acc):\n",
    "            max_temp_acc=np.max(evaluation[idx].history['val_accuracy'])\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        loss_vec_train= evaluation[idx].history['loss']\n",
    "        loss_vec_test= evaluation[idx].history['val_loss']\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "        plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss across epochs for fold: '+str(idx))\n",
    "        plt.ylim([min_temp_loss, max_temp_loss])\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(dir1+\"/loss_kfold_\"+str(idx))\n",
    "        plt.close()\n",
    "\n",
    "    #plot accuracy and loss function across epochs\n",
    "    epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        loss_vec_train= evaluation[idx].history['accuracy']\n",
    "        loss_vec_test= evaluation[idx].history['val_accuracy']\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "        plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy across epochs for fold: '+str(idx))\n",
    "        plt.ylim([min_temp_acc, max_temp_acc])\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(dir1+\"/accuracy_kfold_\"+str(idx))\n",
    "        plt.close()\n",
    "\n",
    "    #SALVO VARIABILI STATISTICHE E MODELLO IN MODO DA PLOTTARLO\n",
    "\n",
    "#         acc_temp=[]\n",
    "    accuratezza=np.zeros(n_classes*2)\n",
    "#         for idx in range(n_folds):\n",
    "#             acc_temp.append(PERFORMANCE[idx][\"accuracy\"])\n",
    "    accuratezza[0]=(np.mean(validation_acc))\n",
    "    accuratezza[1]=(statistics.pstdev(validation_acc))\n",
    "\n",
    "\n",
    "    precisione=[]\n",
    "    recall=[]\n",
    "    f1_score=[]\n",
    "    support=[]\n",
    "    for classe in range(numero_classi):\n",
    "        precision_temp=[]\n",
    "        recall_temp=[]\n",
    "        f1_score_temp=[]\n",
    "        support_temp=[]\n",
    "        for idx in range(n_folds):\n",
    "            precision_temp.append(PERFORMANCE[idx][str(classe)][\"precision\"])\n",
    "            recall_temp.append(PERFORMANCE[idx][str(classe)][\"recall\"])\n",
    "            f1_score_temp.append(PERFORMANCE[idx][str(classe)][\"f1-score\"])\n",
    "            support_temp.append(PERFORMANCE[idx][str(classe)][\"support\"])\n",
    "\n",
    "        precisione.append(np.mean(precision_temp))\n",
    "        precisione.append(statistics.pstdev(precision_temp))\n",
    "        recall.append(np.mean(recall_temp))\n",
    "        recall.append(statistics.pstdev(recall_temp))\n",
    "        f1_score.append(np.mean(f1_score_temp))\n",
    "        f1_score.append(statistics.pstdev(f1_score_temp))    \n",
    "        support.append(np.mean(support_temp))\n",
    "        support.append(statistics.pstdev(support_temp)) \n",
    "\n",
    "        sio.savemat(dir0+\"/precisione.mat\", {\"array\":precisione})\n",
    "        sio.savemat(dir0+\"/recall.mat\", {\"array\":recall})\n",
    "        sio.savemat(dir0+\"/f1_score.mat\", {\"array\":f1_score})\n",
    "        sio.savemat(dir0+\"/support.mat\", {\"array\":support})\n",
    "\n",
    "#     sommario=[]\n",
    "#     sommario=np.vstack((accuratezza,precisione,recall,f1_score,recall))\n",
    "#     sio.savemat(dir0+\"/sommario.mat\", {\"array\": sommario})\n",
    "\n",
    "    gianfranco=model.predict(X)\n",
    "    gianfranco2=np.argmax(gianfranco, axis=1)\n",
    "    sio.savemat(dir0+\"/predizione_totale.mat\", {\"array\": gianfranco2})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e98f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER SUB & PROTOCOL\n",
    "# SUBJECTS=[6,7,8,9,11,12,13,14,16,17]\n",
    "# for temp_sub in SUBJECTS:\n",
    "CLASSES= [2,3,5,10,50]\n",
    "#questa va applicata a for loop di subject che deve essere il pi첫 esterno \n",
    "#     sub=\"{:02d}\".format(temp_sub)\n",
    "\n",
    "dir00= os.path.join(\"FINAL_fuzzy_regressor/MinMax_per_sub_protocol/output\", \"all\")\n",
    "os.mkdir(dir00)\n",
    "for n_classes in CLASSES:\n",
    "    #Define loss function\n",
    "    loss_fn= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    LABELS= list(range(0,n_classes))\n",
    "\n",
    "    if n_classes==50:\n",
    "        numero_classi=5\n",
    "    else:\n",
    "        numero_classi=n_classes\n",
    "\n",
    "    evaluation=[]\n",
    "    iteration=[]\n",
    "    confusion_matrix_x_test=[]\n",
    "    confusion_matrix_y_test= []\n",
    "    validation_acc=[]\n",
    "    PERFORMANCE=[]\n",
    "\n",
    "#     print(\"SUBJECT: \"+ str(sub))\n",
    "    print(\"N_CLASSES: \"+ str(n_classes))\n",
    "\n",
    "    X=np.load(\"input/RAW_regression_all_subs_float32.npy\")\n",
    "    y= sio.loadmat(\"FINAL_fuzzy_regressor/MinMax_per_sub_protocol/input/Compiled_rightSMA_all_scaled_per_sub_\"+str(n_classes)+\"class_balanced.mat\")[\"compiled_rightSMA_labels_all\"]\n",
    "\n",
    "    X= X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "    y=y.flatten()\n",
    "    print(\"Shape for X: \" + str(X.shape))\n",
    "    print(\"Shape for y: \"+str(y.shape))\n",
    "\n",
    "    X= X[762:,:,:,:]\n",
    "    y= y[762:]\n",
    "    print(\"New shape for X: \" + str(X.shape))\n",
    "    print(\"New shape for y: \"+str(y.shape))\n",
    "\n",
    "    dir0= os.path.join(dir00, str(n_classes)+\"_class\")\n",
    "    os.mkdir(dir0)\n",
    "    dir1=os.path.join(dir0, \"comparison\")\n",
    "    dir2=os.path.join(dir0, \"temporal_convolution\")\n",
    "    dir3=os.path.join(dir0, \"spatial_convolution\")\n",
    "    os.mkdir(dir1)\n",
    "    os.mkdir(dir2)\n",
    "    os.mkdir(dir3)\n",
    "\n",
    "    ################################################################################################################################################################################################################################################################################\n",
    "\n",
    "    n_folds = 5\n",
    "    seed = 24\n",
    "    shuffle_test = True\n",
    "    EPOCHS=250\n",
    "    DROPOUT_TEST=0.25\n",
    "    \n",
    "    kfold = KFold(n_splits = n_folds, shuffle = shuffle_test, random_state = seed)\n",
    "    count=0\n",
    "    sommatoria=0\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        count=count+1\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        print(\"Train index for this split: \"+ str(train_index)) \n",
    "        print(\"Number of samples for train set: \"+str(train_index.shape[0]))\n",
    "        print(\"Test index for this split: \"+ str(test_index))\n",
    "        print(\"Number of samples for test set: \"+str(test_index.shape[0]))\n",
    "\n",
    "        # Define the model architecture - \n",
    "\n",
    "        model=Sequential()\n",
    "\n",
    "        ##################################################################\n",
    "\n",
    "        model.add(Conv2D(8, (1, 64), padding = 'same',\n",
    "                                       input_shape = (N_chan, N_samples_long, 1),\n",
    "                                       use_bias = False, name=\"temporal_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_1\"))\n",
    "        model.add(DepthwiseConv2D((31, 1), use_bias = False, \n",
    "                                       depth_multiplier = 2,\n",
    "                                       depthwise_constraint = max_norm(1.), name=\"spatial_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_2\"))\n",
    "        model.add(Activation('elu', name=\"activation_1\"))\n",
    "        model.add(AveragePooling2D((1, 4), name=\"pooling_layer_1\"))\n",
    "        model.add(Dropout(0.5, name=\"dropout_1\"))\n",
    "\n",
    "        model.add(SeparableConv2D(16, (1, 16),\n",
    "                                       use_bias = False, padding = 'same', name=\"separable_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_3\"))\n",
    "        model.add(Activation('elu', name=\"activation_2\"))\n",
    "        model.add(AveragePooling2D((1, 8), name=\"pooling_layer_2\"))\n",
    "        model.add(Dropout(DROPOUT_TEST, name=\"drpout_2\")) #QUI DROPOUT E' LASCIATO A 0.5 come in eegnet paper\n",
    "\n",
    "        model.add(Flatten(name = 'flatten'))\n",
    "\n",
    "        model.add(Dense(numero_classi, name = 'dense', \n",
    "                                 kernel_constraint = max_norm(0.25)))\n",
    "        model.add(Activation('softmax', name = 'softmax'))\n",
    "\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer= optimizers.Adam(\n",
    "        learning_rate= 1e-4,\n",
    "        weight_decay= 0\n",
    "        )\n",
    "        model.compile(optimizer=optimizer,\n",
    "                       loss=loss_fn,\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "        evaluation.append(model.fit(X_train, y_train,\n",
    "                  epochs=EPOCHS, \n",
    "                  validation_data=(X_test, y_test), \n",
    "                  verbose=0, workers=1)\n",
    "                     )\n",
    "\n",
    "        # Iteration = fold, i am just saving the model for that fold\n",
    "        iteration.append(model)\n",
    "\n",
    "        confusion_matrix_x_test.append(X_test)\n",
    "        confusion_matrix_y_test.append(y_test)\n",
    "\n",
    "        #Plotting confusion matrix\n",
    "        pred=model.predict(X_test)\n",
    "        y_test_pred= np.argmax(pred, axis=1)\n",
    "\n",
    "        confusion_matrix= metrics.confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "        plt.figure()\n",
    "        metrics.ConfusionMatrixDisplay(confusion_matrix).plot()\n",
    "        plt.savefig(dir1+\"/confusion_matrix_kfold_\"+str(count))\n",
    "        plt.close()\n",
    "\n",
    "        validation_acc.append(np.sum(y_test==y_test_pred)/y_test.shape[0])\n",
    "\n",
    "        PERFORMANCE.append(classification_report(y_test, y_test_pred, labels=LABELS, output_dict=True))\n",
    "\n",
    "        #Salvo risultati di singolo fold\n",
    "        sio.savemat(dir1+\"/y_pred_test_kfold\"+str(count), {\"array\": y_test_pred})\n",
    "        sio.savemat(dir1+\"/y_test_kfold\"+str(count), {\"array\": y_test})\n",
    "\n",
    "\n",
    "        # PLOTTO FILTRI TEMPORALI E SPAZIALI E LI SALVO\n",
    "        var= (model.get_layer(\"temporal_conv\").weights)\n",
    "        for lallo in range(8):\n",
    "            plt.figure()\n",
    "            plt.title(\"temp_conv_\"+str(lallo))\n",
    "            plt.plot(var[0][0,:,0][:,lallo]) #this way i access the temporal filters, cambiando ultimo zero\n",
    "            plt.savefig(dir2+\"/temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo))\n",
    "            temp= (var[0][0,:,0][:,lallo]).numpy()\n",
    "            sio.savemat(dir2+\"/temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo), {\"array\": temp})\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        var_2= (model.get_layer(\"spatial_conv\").weights)\n",
    "        reshaped_var_2=tf.reshape(var_2[0][:,0,:,:],(31,16))\n",
    "        for lallo in range(16):\n",
    "            nump= reshaped_var_2[:,lallo].numpy()\n",
    "            sio.savemat(dir3+\"/spat_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo), {\"array\": nump})\n",
    "\n",
    "    ###################################################################################################################\n",
    "\n",
    "    #plot accuracy and loss function across epochs\n",
    "    epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "    min_temp_loss=10\n",
    "    min_temp_acc=10\n",
    "    max_temp_loss=0\n",
    "    max_temp_acc=0\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        if (np.min(evaluation[idx].history['loss'])<min_temp_loss):\n",
    "            min_temp_loss=np.min(evaluation[idx].history['loss'])\n",
    "        if (np.min(evaluation[idx].history['val_loss'])<min_temp_loss):\n",
    "            min_temp_loss=np.min(evaluation[idx].history['val_loss'])\n",
    "        if (np.max(evaluation[idx].history['loss'])>max_temp_loss):\n",
    "            max_temp_loss=np.max(evaluation[idx].history['loss'])\n",
    "        if (np.max(evaluation[idx].history['val_loss'])>max_temp_loss):\n",
    "            max_temp_loss=np.max(evaluation[idx].history['val_loss'])\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        if (np.min(evaluation[idx].history['accuracy'])<min_temp_acc):\n",
    "            min_temp_acc=np.min(evaluation[idx].history['accuracy'])\n",
    "        if (np.min(evaluation[idx].history['val_accuracy'])<min_temp_acc):\n",
    "            min_temp_acc=np.min(evaluation[idx].history['val_accuracy'])\n",
    "        if (np.max(evaluation[idx].history['accuracy'])>max_temp_acc):\n",
    "            max_temp_acc=np.max(evaluation[idx].history['accuracy'])\n",
    "        if (np.max(evaluation[idx].history['val_accuracy'])>max_temp_acc):\n",
    "            max_temp_acc=np.max(evaluation[idx].history['val_accuracy'])\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        loss_vec_train= evaluation[idx].history['loss']\n",
    "        loss_vec_test= evaluation[idx].history['val_loss']\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "        plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss across epochs for fold: '+str(idx))\n",
    "        plt.ylim([min_temp_loss, max_temp_loss])\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(dir1+\"/loss_kfold_\"+str(idx))\n",
    "        plt.close()\n",
    "\n",
    "    #plot accuracy and loss function across epochs\n",
    "    epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        loss_vec_train= evaluation[idx].history['accuracy']\n",
    "        loss_vec_test= evaluation[idx].history['val_accuracy']\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "        plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy across epochs for fold: '+str(idx))\n",
    "        plt.ylim([min_temp_acc, max_temp_acc])\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(dir1+\"/accuracy_kfold_\"+str(idx))\n",
    "        plt.close()\n",
    "\n",
    "    #SALVO VARIABILI STATISTICHE E MODELLO IN MODO DA PLOTTARLO\n",
    "\n",
    "#         acc_temp=[]\n",
    "    accuratezza=np.zeros(n_classes*2)\n",
    "#         for idx in range(n_folds):\n",
    "#             acc_temp.append(PERFORMANCE[idx][\"accuracy\"])\n",
    "    accuratezza[0]=(np.mean(validation_acc))\n",
    "    accuratezza[1]=(statistics.pstdev(validation_acc))\n",
    "\n",
    "\n",
    "    precisione=[]\n",
    "    recall=[]\n",
    "    f1_score=[]\n",
    "    support=[]\n",
    "    for classe in range(numero_classi):\n",
    "        precision_temp=[]\n",
    "        recall_temp=[]\n",
    "        f1_score_temp=[]\n",
    "        support_temp=[]\n",
    "        for idx in range(n_folds):\n",
    "            precision_temp.append(PERFORMANCE[idx][str(classe)][\"precision\"])\n",
    "            recall_temp.append(PERFORMANCE[idx][str(classe)][\"recall\"])\n",
    "            f1_score_temp.append(PERFORMANCE[idx][str(classe)][\"f1-score\"])\n",
    "            support_temp.append(PERFORMANCE[idx][str(classe)][\"support\"])\n",
    "\n",
    "        precisione.append(np.mean(precision_temp))\n",
    "        precisione.append(statistics.pstdev(precision_temp))\n",
    "        recall.append(np.mean(recall_temp))\n",
    "        recall.append(statistics.pstdev(recall_temp))\n",
    "        f1_score.append(np.mean(f1_score_temp))\n",
    "        f1_score.append(statistics.pstdev(f1_score_temp))    \n",
    "        support.append(np.mean(support_temp))\n",
    "        support.append(statistics.pstdev(support_temp)) \n",
    "\n",
    "        sio.savemat(dir0+\"/precisione.mat\", {\"array\":precisione})\n",
    "        sio.savemat(dir0+\"/recall.mat\", {\"array\":recall})\n",
    "        sio.savemat(dir0+\"/f1_score.mat\", {\"array\":f1_score})\n",
    "        sio.savemat(dir0+\"/support.mat\", {\"array\":support})\n",
    "\n",
    "#     sommario=[]\n",
    "#     sommario=np.vstack((accuratezza,precisione,recall,f1_score,recall))\n",
    "#     sio.savemat(dir0+\"/sommario.mat\", {\"array\": sommario})\n",
    "\n",
    "    gianfranco=model.predict(X)\n",
    "    gianfranco2=np.argmax(gianfranco, axis=1)\n",
    "    sio.savemat(dir0+\"/predizione_totale.mat\", {\"array\": gianfranco2})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0120227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER RUN \n",
    "# SUBJECTS=[3,4,5,6,7,8,9,11,12,13,14,16,17]\n",
    "# for temp_sub in SUBJECTS:\n",
    "CLASSES= [2,3,5,10,50]\n",
    "#questa va applicata a for loop di subject che deve essere il pi첫 esterno \n",
    "# sub=\"{:02d}\".format(temp_sub)\n",
    "\n",
    "dir00= os.path.join(\"FINAL_fuzzy_regressor/MinMax_per_run/output\", \"all\")\n",
    "os.mkdir(dir00)\n",
    "for n_classes in CLASSES:\n",
    "    #Define loss function\n",
    "    loss_fn= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    LABELS= list(range(0,n_classes))\n",
    "\n",
    "    if n_classes==50:\n",
    "        numero_classi=5\n",
    "    else:\n",
    "        numero_classi=n_classes\n",
    "\n",
    "    evaluation=[]\n",
    "    iteration=[]\n",
    "    confusion_matrix_x_test=[]\n",
    "    confusion_matrix_y_test= []\n",
    "    validation_acc=[]\n",
    "    PERFORMANCE=[]\n",
    "\n",
    "    X=np.load(\"input/RAW_regression_all_subs_float32.npy\")\n",
    "    y= sio.loadmat(\"FINAL_fuzzy_regressor/MinMax_per_run/input/Compiled_rightSMA_all_scaled_per_sub_\"+str(n_classes)+\"class_balanced.mat\")[\"compiled_rightSMA_labels_all\"]\n",
    "\n",
    "    X= X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "    y=y.flatten()\n",
    "    print(\"Shape for X: \" + str(X.shape))\n",
    "    print(\"Shape for y: \"+str(y.shape))\n",
    "\n",
    "    X= X[762:,:,:,:]\n",
    "    y= y[762:]\n",
    "    print(\"New shape for X: \" + str(X.shape))\n",
    "    print(\"New shape for y: \"+str(y.shape))\n",
    "\n",
    "    dir0= os.path.join(dir00, str(n_classes)+\"_class\")\n",
    "    os.mkdir(dir0)\n",
    "    dir1=os.path.join(dir0, \"comparison\")\n",
    "    dir2=os.path.join(dir0, \"temporal_convolution\")\n",
    "    dir3=os.path.join(dir0, \"spatial_convolution\")\n",
    "    os.mkdir(dir1)\n",
    "    os.mkdir(dir2)\n",
    "    os.mkdir(dir3)\n",
    "\n",
    "    ################################################################################################################################################################################################################################################################################\n",
    "\n",
    "    n_folds = 5\n",
    "#         seed = 21\n",
    "    shuffle_test = True\n",
    "    EPOCHS=250\n",
    "    DROPOUT_TEST=0.25\n",
    "\n",
    "    kfold = KFold(n_splits = n_folds, shuffle = shuffle_test)\n",
    "    count=0\n",
    "    sommatoria=0\n",
    "\n",
    "    for train_index, test_index in kfold.split(X):\n",
    "        count=count+1\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        print(\"Train index for this split: \"+ str(train_index)) \n",
    "        print(\"Number of samples for train set: \"+str(train_index.shape[0]))\n",
    "        print(\"Test index for this split: \"+ str(test_index))\n",
    "        print(\"Number of samples for test set: \"+str(test_index.shape[0]))\n",
    "\n",
    "        # Define the model architecture - \n",
    "\n",
    "        model=Sequential()\n",
    "\n",
    "        ##################################################################\n",
    "\n",
    "        model.add(Conv2D(8, (1, 64), padding = 'same',\n",
    "                                       input_shape = (N_chan, N_samples_long, 1),\n",
    "                                       use_bias = False, name=\"temporal_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_1\"))\n",
    "        model.add(DepthwiseConv2D((31, 1), use_bias = False, \n",
    "                                       depth_multiplier = 2,\n",
    "                                       depthwise_constraint = max_norm(1.), name=\"spatial_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_2\"))\n",
    "        model.add(Activation('elu', name=\"activation_1\"))\n",
    "        model.add(AveragePooling2D((1, 4), name=\"pooling_layer_1\"))\n",
    "        model.add(Dropout(0.5, name=\"dropout_1\"))\n",
    "\n",
    "        model.add(SeparableConv2D(16, (1, 16),\n",
    "                                       use_bias = False, padding = 'same', name=\"separable_conv\"))\n",
    "        model.add(BatchNormalization(name=\"batchnorm_3\"))\n",
    "        model.add(Activation('elu', name=\"activation_2\"))\n",
    "        model.add(AveragePooling2D((1, 8), name=\"pooling_layer_2\"))\n",
    "        model.add(Dropout(DROPOUT_TEST, name=\"drpout_2\")) #QUI DROPOUT E' LASCIATO A 0.5 come in eegnet paper\n",
    "\n",
    "        model.add(Flatten(name = 'flatten'))\n",
    "\n",
    "        model.add(Dense(numero_classi, name = 'dense', \n",
    "                                 kernel_constraint = max_norm(0.25)))\n",
    "        model.add(Activation('softmax', name = 'softmax'))\n",
    "\n",
    "\n",
    "        # Define the optimizer\n",
    "        optimizer= optimizers.Adam(\n",
    "        learning_rate= 1e-4,\n",
    "        weight_decay= 0\n",
    "        )\n",
    "        model.compile(optimizer=optimizer,\n",
    "                       loss=loss_fn,\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "        evaluation.append(model.fit(X_train, y_train,\n",
    "                  epochs=EPOCHS, \n",
    "                  validation_data=(X_test, y_test), \n",
    "                  verbose=0, workers=1)\n",
    "                     )\n",
    "\n",
    "        # Iteration = fold, i am just saving the model for that fold\n",
    "        iteration.append(model)\n",
    "\n",
    "        confusion_matrix_x_test.append(X_test)\n",
    "        confusion_matrix_y_test.append(y_test)\n",
    "\n",
    "        #Plotting confusion matrix\n",
    "        pred=model.predict(X_test)\n",
    "        y_test_pred= np.argmax(pred, axis=1)\n",
    "\n",
    "        confusion_matrix= metrics.confusion_matrix(y_test, y_test_pred, normalize='true')\n",
    "        plt.figure()\n",
    "        metrics.ConfusionMatrixDisplay(confusion_matrix).plot()\n",
    "        plt.savefig(dir1+\"/confusion_matrix_kfold_\"+str(count))\n",
    "        plt.close()\n",
    "\n",
    "        validation_acc.append(np.sum(y_test==y_test_pred)/y_test.shape[0])\n",
    "\n",
    "        PERFORMANCE.append(classification_report(y_test, y_test_pred, labels=LABELS, output_dict=True))\n",
    "\n",
    "        #Salvo risultati di singolo fold\n",
    "        sio.savemat(dir1+\"/y_pred_test_kfold\"+str(count), {\"array\": y_test_pred})\n",
    "        sio.savemat(dir1+\"/y_test_kfold\"+str(count), {\"array\": y_test})\n",
    "\n",
    "\n",
    "        # PLOTTO FILTRI TEMPORALI E SPAZIALI E LI SALVO\n",
    "        var= (model.get_layer(\"temporal_conv\").weights)\n",
    "        for lallo in range(8):\n",
    "            plt.figure()\n",
    "            plt.title(\"temp_conv_\"+str(lallo))\n",
    "            plt.plot(var[0][0,:,0][:,lallo]) #this way i access the temporal filters, cambiando ultimo zero\n",
    "            plt.savefig(dir2+\"/temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo))\n",
    "            temp= (var[0][0,:,0][:,lallo]).numpy()\n",
    "            sio.savemat(dir2+\"/temp_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo), {\"array\": temp})\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        var_2= (model.get_layer(\"spatial_conv\").weights)\n",
    "        reshaped_var_2=tf.reshape(var_2[0][:,0,:,:],(31,16))\n",
    "        for lallo in range(16):\n",
    "            nump= reshaped_var_2[:,lallo].numpy()\n",
    "            sio.savemat(dir3+\"/spat_conv_kfold_\"+str(count)+\"_filter_\"+str(lallo), {\"array\": nump})\n",
    "\n",
    "    ###################################################################################################################\n",
    "\n",
    "    #plot accuracy and loss function across epochs\n",
    "    epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "    min_temp_loss=10\n",
    "    min_temp_acc=10\n",
    "    max_temp_loss=0\n",
    "    max_temp_acc=0\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        if (np.min(evaluation[idx].history['loss'])<min_temp_loss):\n",
    "            min_temp_loss=np.min(evaluation[idx].history['loss'])\n",
    "        if (np.min(evaluation[idx].history['val_loss'])<min_temp_loss):\n",
    "            min_temp_loss=np.min(evaluation[idx].history['val_loss'])\n",
    "        if (np.max(evaluation[idx].history['loss'])>max_temp_loss):\n",
    "            max_temp_loss=np.max(evaluation[idx].history['loss'])\n",
    "        if (np.max(evaluation[idx].history['val_loss'])>max_temp_loss):\n",
    "            max_temp_loss=np.max(evaluation[idx].history['val_loss'])\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        if (np.min(evaluation[idx].history['accuracy'])<min_temp_acc):\n",
    "            min_temp_acc=np.min(evaluation[idx].history['accuracy'])\n",
    "        if (np.min(evaluation[idx].history['val_accuracy'])<min_temp_acc):\n",
    "            min_temp_acc=np.min(evaluation[idx].history['val_accuracy'])\n",
    "        if (np.max(evaluation[idx].history['accuracy'])>max_temp_acc):\n",
    "            max_temp_acc=np.max(evaluation[idx].history['accuracy'])\n",
    "        if (np.max(evaluation[idx].history['val_accuracy'])>max_temp_acc):\n",
    "            max_temp_acc=np.max(evaluation[idx].history['val_accuracy'])\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        loss_vec_train= evaluation[idx].history['loss']\n",
    "        loss_vec_test= evaluation[idx].history['val_loss']\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "        plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss across epochs for fold: '+str(idx))\n",
    "        plt.ylim([min_temp_loss, max_temp_loss])\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(dir1+\"/loss_kfold_\"+str(idx))\n",
    "        plt.close()\n",
    "\n",
    "    #plot accuracy and loss function across epochs\n",
    "    epoch_vec=np.linspace(1,EPOCHS,EPOCHS)\n",
    "\n",
    "    for idx in range(n_folds):\n",
    "        loss_vec_train= evaluation[idx].history['accuracy']\n",
    "        loss_vec_test= evaluation[idx].history['val_accuracy']\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_vec,loss_vec_test,'b-', label= 'test');\n",
    "        plt.plot(epoch_vec,loss_vec_train,'r-', label='train');\n",
    "\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy across epochs for fold: '+str(idx))\n",
    "        plt.ylim([min_temp_acc, max_temp_acc])\n",
    "        plt.legend()\n",
    "\n",
    "        plt.savefig(dir1+\"/accuracy_kfold_\"+str(idx))\n",
    "        plt.close()\n",
    "\n",
    "    #SALVO VARIABILI STATISTICHE E MODELLO IN MODO DA PLOTTARLO\n",
    "\n",
    "#         acc_temp=[]\n",
    "    accuratezza=np.zeros(n_classes*2)\n",
    "#         for idx in range(n_folds):\n",
    "#             acc_temp.append(PERFORMANCE[idx][\"accuracy\"])\n",
    "    accuratezza[0]=(np.mean(validation_acc))\n",
    "    accuratezza[1]=(statistics.pstdev(validation_acc))\n",
    "\n",
    "\n",
    "    precisione=[]\n",
    "    recall=[]\n",
    "    f1_score=[]\n",
    "    support=[]\n",
    "    for classe in range(numero_classi):\n",
    "        precision_temp=[]\n",
    "        recall_temp=[]\n",
    "        f1_score_temp=[]\n",
    "        support_temp=[]\n",
    "        for idx in range(n_folds):\n",
    "            precision_temp.append(PERFORMANCE[idx][str(classe)][\"precision\"])\n",
    "            recall_temp.append(PERFORMANCE[idx][str(classe)][\"recall\"])\n",
    "            f1_score_temp.append(PERFORMANCE[idx][str(classe)][\"f1-score\"])\n",
    "            support_temp.append(PERFORMANCE[idx][str(classe)][\"support\"])\n",
    "\n",
    "        precisione.append(np.mean(precision_temp))\n",
    "        precisione.append(statistics.pstdev(precision_temp))\n",
    "        recall.append(np.mean(recall_temp))\n",
    "        recall.append(statistics.pstdev(recall_temp))\n",
    "        f1_score.append(np.mean(f1_score_temp))\n",
    "        f1_score.append(statistics.pstdev(f1_score_temp))    \n",
    "        support.append(np.mean(support_temp))\n",
    "        support.append(statistics.pstdev(support_temp)) \n",
    "        \n",
    "        sio.savemat(dir0+\"/precisione.mat\", {\"array\":precisione})\n",
    "        sio.savemat(dir0+\"/recall.mat\", {\"array\":recall})\n",
    "        sio.savemat(dir0+\"/f1_score.mat\", {\"array\":f1_score})\n",
    "        sio.savemat(dir0+\"/support.mat\", {\"array\":support})\n",
    "\n",
    "#     sommario=[]\n",
    "#     sommario=np.vstack((accuratezza,precisione,recall,f1_score,recall))\n",
    "#     sio.savemat(dir0+\"/sommario.mat\", {\"array\": sommario})\n",
    "\n",
    "    gianfranco=model.predict(X)\n",
    "    gianfranco2=np.argmax(gianfranco, axis=1)\n",
    "    sio.savemat(dir0+\"/predizione_totale.mat\", {\"array\": gianfranco2})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
